{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveen76/Loan-Approval-Classifier-Model-using-Non-Linear-Models/blob/main/End_to_end_Loan_Approval_Classifier_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Import libraries and data"
      ],
      "metadata": {
        "id": "34bvSz4clv14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxRu0KZ8Kwn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1abb968-b2f9-466a-d3ff-cbf90b003b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Loan-Approval-Classifier-Model-using-Non-Linear-Models'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (126/126), done.\u001b[K\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Praveen76/Loan-Approval-Classifier-Model-using-Non-Linear-Models.git\n",
        "%cd Loan-Approval-Classifier-Model-using-Non-Linear-Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UyGL0WLz-17"
      },
      "outputs": [],
      "source": [
        "!pip install shap xgboost plotly scikit-plot seaborn statsmodels tabulate matplotlib imblearn nbformat>=4.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "# Print Python version\n",
        "print(\"Python version\")\n",
        "print(sys.version)\n",
        "print(\"\")\n",
        "\n",
        "# Print installed package versions\n",
        "print(\"Installed package versions\")\n",
        "for module in sys.modules:\n",
        "    try:\n",
        "        version = sys.modules[module].__version__\n",
        "        print(f\"{module}: {version}\")\n",
        "    except AttributeError:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "tvOXyaXIciem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Import libraries & load Data"
      ],
      "metadata": {
        "id": "EuXQZ7oMa-gr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIeo7svkpZeW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy.stats as stats\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from tabulate import tabulate\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73thl28tuoX7"
      },
      "outputs": [],
      "source": [
        "loan_original=pd.read_csv('./data.csv',parse_dates=['DateAdded','ApprovalDate'])\n",
        "loan_original=loan_original.infer_objects()\n",
        "\n",
        "loan_original.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original.columns"
      ],
      "metadata": {
        "id": "uRGmRGMWntyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original.LoanPurpose.value_counts()"
      ],
      "metadata": {
        "id": "KPjwPOcOnwOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original.LoanType.value_counts()"
      ],
      "metadata": {
        "id": "gkuhByZfn6eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMIGeLnVK9Ke"
      },
      "source": [
        "# Step 1: Data Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAKaDCycx0F0"
      },
      "outputs": [],
      "source": [
        "loan_original.describe(include = \"all\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhmiFlS4_XTO"
      },
      "outputs": [],
      "source": [
        "#Check for duplicate values\n",
        "loan_original[loan_original['LeadID'].duplicated(keep = False) == True].sort_values(['LeadID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwKu61BpyJar"
      },
      "outputs": [],
      "source": [
        "#Check for duplicate values\n",
        "duplicate_records = loan_original[loan_original['LoanNumber'].duplicated(keep = False) == True].sort_values(['LoanNumber'])\n",
        "print(f\"Number of Duplicate records: {duplicate_records.shape}\")\n",
        "duplicate_records.to_csv(\"duplicate_records.csv\")\n",
        "\n",
        "# Duplicate records found. Hence,\n",
        "loan_original.drop_duplicates(subset='LoanNumber', keep='first', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WvoLnvAyRGF"
      },
      "outputs": [],
      "source": [
        "loan_original.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z1_dOV2ymrN"
      },
      "outputs": [],
      "source": [
        "loan_original.columns = loan_original.columns.str.replace(' ', '')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu2kqD-ByqfZ"
      },
      "outputs": [],
      "source": [
        "loan_original.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which identifier column to drop\n",
        "loan_original.LoanNumber.nunique(), loan_original.LeadID.nunique()"
      ],
      "metadata": {
        "id": "Y9pmzlwgE-Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzYkNjVyysF7"
      },
      "outputs": [],
      "source": [
        "## Drop unnecessary columns\n",
        "cols =['BranchCode','LoanNumber']\n",
        "loan_original = loan_original.drop(cols, axis=1)\n",
        "loan_original.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajz3QIgjFynF"
      },
      "outputs": [],
      "source": [
        "loan_original.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data abnormality check\n",
        "\n",
        "clms=['DTI','BorrowerYearsInSchool','BorrowerTotalMonthlyIncome','CoBorrowerTotalMonthlyIncome','TotalLoanAmount']\n",
        "\n",
        "for col in clms:\n",
        "    print('Negative values for column ',col,' :',loan_original[loan_original[col]<0].shape[0])"
      ],
      "metadata": {
        "id": "ue3vHQRGcysN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check loan_original range on dates\n",
        "print(f'DateAdded min :  {loan_original.DateAdded.min().date()} :: DateAdded max: :,{loan_original.DateAdded.max().date()}')\n",
        "\n",
        "\n",
        "loan_original[loan_original.DateAdded>='2017-01-01'].sort_values(by=['DateAdded'])['DateAdded'].shape"
      ],
      "metadata": {
        "id": "YEajd8l_cyok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original.shape"
      ],
      "metadata": {
        "id": "5FPlvECRc7Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if both loan applicants' monthly income is 0\n",
        "cols=['BorrowerTotalMonthlyIncome','CoBorrowerTotalMonthlyIncome']\n",
        "print(loan_original[(loan_original[cols] <= 0).all(1)].shape)"
      ],
      "metadata": {
        "id": "BLayHwYucybp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'Approved'\n",
        "\n",
        "#Create Approved Date column\n",
        "loan_original['Approved']='Yes'\n",
        "loan_original['Approved'][loan_original.ApprovalDate.isna()]='No'\n",
        "\n",
        "print('Loan Approved :',loan_original[((loan_original[cols] <= 0).all(1)) & (loan_original['Approved']=='Yes')].shape)\n"
      ],
      "metadata": {
        "id": "TN32Alu9c_sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original.Approved.value_counts(normalize=False)"
      ],
      "metadata": {
        "id": "tNjBMYpK32VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of approved and rejected loan applications\n",
        "loan_original.Approved.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "9YNDMxuN3xVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original[(loan_original[cols] <= 0).all(1)]\n"
      ],
      "metadata": {
        "id": "KHTpfX7Wc_ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if BorrowerAge age is 18\n",
        "\n",
        "print(f\"Number of records with BorrowerAge<=18 :: {loan_original[loan_original['BorrowerAge'] <= 18].shape}\")\n",
        "\n",
        "\n",
        "loan_original=loan_original[~(loan_original['BorrowerAge'] <= 18)] #Chuck these records\n",
        "loan_original[loan_original['BorrowerAge'] <= 18]"
      ],
      "metadata": {
        "id": "z54aLxWgc_jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: EDA ( Exploratory Data Analysis)"
      ],
      "metadata": {
        "id": "agV1G_J5GuKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.1: Hypothesis:\n",
        "1. Applicants with high incomes should have more chances of loan approval\n",
        "2. Loan approval should also depend on the loan amount. If the loan amount is less, the chances of loan approval should be high.\n",
        "3. Loan approval should also depend on the Loan Purpose. Refinance Loan approval should be higher, considering it's just the top-up on ongoing loan.\n",
        "4. Loan approval should also depend on the Total Loan Amount. If the loan amount is less, the chances of loan approval should be high.\n",
        "5. Refinance Loan approval should be higher, considering it's just the top-up on an ongoing loan."
      ],
      "metadata": {
        "id": "W6M0gsSSu4mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check different categories of Loan Purpose and Loan Types"
      ],
      "metadata": {
        "id": "RRsdXnj7qL5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Strip off white space\n",
        "loan_original.LoanPurpose=[entry.strip() for entry in loan_original.LoanPurpose]\n",
        "loan_original.LoanType=[entry.strip() for entry in loan_original.LoanType]"
      ],
      "metadata": {
        "id": "w7IzSKcfpYJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_df = loan_original.copy(deep=True)\n",
        "\n",
        "tmp_df['Approved'] = tmp_df['Approved'].replace({'Yes': 1, 'No': 0}).astype(int)\n",
        "tmp_df.groupby(['LoanPurpose'])['Approved'].sum().sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "E90-8jpKojk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verdict: Our hypothesis holds true. Refinance has the highest number of approved loans"
      ],
      "metadata": {
        "id": "VBKXObIc5vP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Refinance Cash-out (4615):**\n",
        "   - **Meaning:** Refinance Cash-out refers to a type of mortgage where the borrower refinances their existing mortgage and takes out additional cash beyond the amount needed to pay off the existing loan.\n",
        "   - **Typical Scenario:** Borrowers may choose this option when they have accumulated equity in their home and want to convert a portion of that equity into cash for purposes such as home improvements, debt consolidation, or other financial needs.\n",
        "\n",
        "2. **Refinance (3519):**\n",
        "   - **Meaning:** Refinance, in a general sense, refers to the process of replacing an existing mortgage with a new one, usually to secure better loan terms or take advantage of changes in interest rates.\n",
        "   - **Typical Scenario:** Borrowers may refinance to obtain a lower interest rate, reduce monthly payments, change from an adjustable-rate to a fixed-rate mortgage, or shorten the loan term.\n",
        "\n",
        "3. **Purchase (1245):**\n",
        "   - **Meaning:** Purchase loans are mortgages used specifically for purchasing a new home.\n",
        "   - **Typical Scenario:** Borrowers obtain purchase loans when they are buying a home. The loan amount is used to fund the acquisition of the property.\n",
        "\n",
        "4. **VA IRRRL (122):**\n",
        "   - **Meaning:** VA IRRRL stands for Veterans Affairs Interest Rate Reduction Refinance Loan. It is a specific type of refinance loan available to eligible veterans.\n",
        "   - **Typical Scenario:** VA IRRRL is designed to help veterans lower their interest rates on existing VA loans. It typically has streamlined approval processes and reduced documentation requirements.\n",
        "\n",
        "5. **FHA Streamlined Refinance (10):**\n",
        "   - **Meaning:** FHA Streamlined Refinance is a refinancing option offered by the Federal Housing Administration (FHA) with simplified requirements compared to a standard refinance.\n",
        "   - **Typical Scenario:** Borrowers with existing FHA loans may choose this option to refinance with minimal paperwork, reduced fees, and often without a new appraisal.\n"
      ],
      "metadata": {
        "id": "Vu5rJvk4p5uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tmp_df.groupby(['LoanType'])['Approved'].sum().sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "mP-eDw6Fojhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Conventional:**\n",
        "   - **Meaning:** Conventional loans are standard mortgage loans that are not insured or guaranteed by any government agency. They conform to the guidelines set by Fannie Mae and Freddie Mac, two government-sponsored enterprises. Conventional loans typically require a higher credit score and a larger down payment compared to government-backed loans.\n",
        "\n",
        "2. **FHA (Federal Housing Administration):**\n",
        "   - **Meaning:** FHA loans are insured by the Federal Housing Administration, a government agency within the U.S. Department of Housing and Urban Development (HUD). These loans are designed to help individuals with lower credit scores or limited financial resources qualify for a mortgage. FHA loans often have more lenient credit and down payment requirements.\n",
        "\n",
        "3. **VA (Department of Veterans Affairs):**\n",
        "   - **Meaning:** VA loans are guaranteed by the Department of Veterans Affairs and are available to eligible veterans, active-duty service members, and certain spouses. These loans aim to support veterans in achieving homeownership by offering favorable terms, including no down payment requirement and competitive interest rates.\n",
        "\n",
        "4. **USDA Rural:**\n",
        "   - **Meaning:** USDA Rural loans are backed by the U.S. Department of Agriculture (USDA) and are designed to encourage homeownership in rural and suburban areas. These loans may offer favorable terms, including low or zero down payment options, to individuals or families purchasing homes in eligible rural locations."
      ],
      "metadata": {
        "id": "y9KgR05HqFeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.2: Find numerical and categorical variables"
      ],
      "metadata": {
        "id": "hsdG-MVWomsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of numerical columns\n",
        "vars_num = [c for c in loan_original.columns if loan_original[c].dtypes!='O' ]#and c!=target]\n",
        "\n",
        "# List of categorical columns\n",
        "vars_cat = [c for c in loan_original.columns if loan_original[c].dtypes=='O']\n",
        "\n",
        "print('Number of numerical variables: {}'.format(len(vars_num)),\":\" , vars_num)\n",
        "\n",
        "print('Number of categorical variables: {}'.format(len(vars_cat)),\":\" , vars_cat)\n"
      ],
      "metadata": {
        "id": "0KdGQHNNG6M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.3: Find missing values in variables\n"
      ],
      "metadata": {
        "id": "bBctS4UJHlw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"% of missing values in num vars :\\n{round(loan_original[vars_num].isnull().mean()*100,2)}\")\n"
      ],
      "metadata": {
        "id": "Y5VLuNF-_qHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of missing values in num vars :\\n{loan_original[vars_num].isnull().sum()}\")\n"
      ],
      "metadata": {
        "id": "wxSN06sCl4kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"% of missing values in cat vars :\\n {round(loan_original[vars_num].isnull().mean()*100,2)}\")\n"
      ],
      "metadata": {
        "id": "R4Sb5emrADSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first in numerical variables\n",
        "print(f\"Number of missing values in num vars :\\n {loan_original[vars_num].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "sNJJKfLNHivA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first in numerical variables\n",
        "print(f\"Number of missing values in cat vars :\\n {loan_original[vars_cat].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "5qjUGgjEHxjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.4: Determine cardinality of categorical variables"
      ],
      "metadata": {
        "id": "uKjN_PMNH9SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique categories in categorical variables\n",
        "loan_original[vars_cat].nunique()"
      ],
      "metadata": {
        "id": "VmkxnB5hH78f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of numerical variables\n",
        "vars_num = ['CreditScore', 'DTI', 'BorrowerAge', 'BorrowerTotalMonthlyIncome']\n",
        "\n",
        "# Create subplots with customized layout\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
        "fig.suptitle('Histograms of Numerical Variables', fontsize=16)\n",
        "\n",
        "# Plot histograms for each variable\n",
        "for i, var in enumerate(vars_num):\n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "    loan_original[var].hist(bins=30, ax=axes[row, col])\n",
        "    axes[row, col].set_title(var)\n",
        "    axes[row, col].set_xlabel('')\n",
        "    axes[row, col].set_ylabel('')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bTpPldOWpJ4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-zPmbehLRbK"
      },
      "outputs": [],
      "source": [
        "loan_original.LoanPurpose.unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Recategorize Loan Purpose column\n",
        "LP = ['Refinance','Purchase']\n",
        "loan_original['LoanPurpose']= [v if v in LP else 'Refinance' for v in loan_original.LoanPurpose]\n",
        "\n",
        "#Calculate difference between current date and DateAdded in days\n",
        "loan_original['Diff']=pd.Timestamp.now().normalize()-loan_original['DateAdded'][loan_original.ApprovalDate.isna()]\n",
        "loan_original['Diff']=[entry.days for entry in loan_original['Diff']]\n",
        "\n",
        "loan_original['Approved']=['No' if ((x>=90) & (y=='Purchase')) else 'Yes' for (x,y) in zip(loan_original.Diff, loan_original.LoanPurpose)]\n",
        "loan_original['Approved']=['No' if ((x>=45) & (y=='Refinance')) else 'Yes' for (x,y) in zip(loan_original.Diff, loan_original.LoanPurpose)]\n",
        "\n",
        "#\n",
        "loan_original['Approved']=loan_original['Approved'].astype('category')\n",
        "loan_original['IsCoBorrowerower']=loan_original['IsCoBorrowerower'].astype('category')\n",
        "\n",
        "loan_original.drop('Diff', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "LP = ['Own','Rent']\n",
        "loan_original['BorrowerOwnRent']= [v if v in LP else 'Own' for v in loan_original.BorrowerOwnRent]\n",
        "loan_original['BorrowerOwnRent'][loan_original['BorrowerOwnRent'].isna()]=loan_original['BorrowerOwnRent'].mode().values\n",
        "\n",
        "loan_original.Approved.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "9P22VEniNH0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classes Proportion: We see that almost 73% loan applications were approved, and 27% loan applications were denied."
      ],
      "metadata": {
        "id": "ej5F6mGDNkzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add Borrower & Co-Borrower's income\n",
        "loan_original['TotalIncome']=loan_original.BorrowerTotalMonthlyIncome + loan_original.CoBorrowerTotalMonthlyIncome\n",
        "# loan_original.drop(['BorrowerTotalMonthlyIncome','CoBorrowerTotalMonthlyIncome'],axis=1,inplace=True)\n",
        "\n",
        "#Checck if Total income is 0\n",
        "\n",
        "loan_original[((loan_original['TotalIncome']<0))]\n",
        "\n",
        "#Regroup LeadSourceGroup\n",
        "LSR = ['Internet', 'TV','Radio','Repeat Client' ]\n",
        "loan_original['LeadSourceGroup'] = [v if v in LSR else 'Other' for v in loan_original.LeadSourceGroup]\n",
        "\n",
        "zips = ['75', '76', '77', '78', '79']\n",
        "loan_original['ZipCode'] = [str(zp)[:2] for zp in loan_original['ZipCode']]\n",
        "loan_original['ZipCode'] = [v if v in zips else 'Other' for v in loan_original.ZipCode]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5_sJl_ssOBNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars_num"
      ],
      "metadata": {
        "id": "iIIG70exdbjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original[(loan_original['TotalIncome'] <= 0) & (loan_original['DTI'] > 0)].shape"
      ],
      "metadata": {
        "id": "5a5s8EoLCYC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impute missing values in DTI\n"
      ],
      "metadata": {
        "id": "gZCsBIQbnl90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"Missing DTI count before imputation:\", loan_original['DTI'].isna().sum())\n",
        "\n",
        "# Select features for imputation (including DTI itself)\n",
        "features = ['DTI', 'CreditScore', 'CLTV', 'BorrowerAge','TotalIncome']\n",
        "\n",
        "# Subset data\n",
        "df_impute = loan_original[features].copy()\n",
        "\n",
        "# Standardize features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df_impute)\n",
        "\n",
        "# Apply KNN imputer (k=5 neighbors by default)\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_imputed_scaled = imputer.fit_transform(df_scaled)\n",
        "\n",
        "# Inverse transform back to original scale\n",
        "df_imputed = pd.DataFrame(scaler.inverse_transform(df_imputed_scaled),\n",
        "                          columns=features)\n",
        "\n",
        "# Replace original DTI with imputed values\n",
        "loan_original['DTI'] = df_imputed['DTI']\n",
        "\n",
        "print(\"Missing DTI count after imputation:\", loan_original['DTI'].isna().sum())\n"
      ],
      "metadata": {
        "id": "XAl--s9Onlod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weqewe"
      ],
      "metadata": {
        "id": "eiQJfNS7no2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# 1. Basic summary stats\n",
        "print(\"Summary of DTI:\")\n",
        "print(loan_original['DTI'].describe())\n",
        "\n",
        "# 2. Check negative values\n",
        "neg_count = (loan_original['DTI'] < 0).sum()\n",
        "print(f\"\\nNegative DTI count: {neg_count}\")\n",
        "\n",
        "# 3. Check extremely large values (say >400%)\n",
        "extreme_count = (loan_original['DTI'] > 400).sum()\n",
        "print(f\"Extreme DTI count (>400): {extreme_count}\")\n",
        "\n",
        "# 4. Check income = 0 but DTI > 0 (likely invalid)\n",
        "invalid_income = loan_original[(loan_original['TotalIncome'] <= 0) & (loan_original['DTI'] > 0)]\n",
        "print(f\"\\nCases with income=0 but positive DTI: {invalid_income.shape[0]}\")\n",
        "\n",
        "# 5. Missing values\n",
        "missing_count = loan_original['DTI'].isna().sum()\n",
        "print(f\"Missing DTI count: {missing_count}\")\n",
        "\n",
        "# 6. Top unusual values (outliers)\n",
        "print(\"\\nTop 10 highest DTI values:\")\n",
        "print(loan_original['DTI'].sort_values(ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "CvhjZOmhCRVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cap DTI values greater than 500 to 500; Impute all DTI>=500 to 500 only\n",
        "import numpy as np\n",
        "\n",
        "# 1. Remove rows where income=0 but DTI>0 (invalid cases)\n",
        "loan_original = loan_original[~((loan_original['BorrowerTotalMonthlyIncome'] <= 0) & (loan_original['DTI'] > 0))]\n",
        "\n",
        "# 2. Cap extreme outliers at 400%\n",
        "loan_original['DTI'] = loan_original['DTI'].clip(upper=400)\n",
        "\n",
        "print(\"Max DTI after cleaning:\", loan_original['DTI'].max())\n",
        "\n",
        "# Check result\n",
        "print(\"Max DTI after capping:\", loan_original['DTI'].max())\n"
      ],
      "metadata": {
        "id": "vbyFvEtoCriW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nums_feats = ['TotalLoanAmount', 'CreditScore', 'CLTV', 'DTI','BorrowerAge', 'BorrowerYearsInSchool',\n",
        "              'BorrowerTotalMonthlyIncome', 'CoBorrowerYearsInSchool', 'CoBorrowerAge','CoBorrowerTotalMonthlyIncome']\n"
      ],
      "metadata": {
        "id": "mBIaNRFcOTP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(loan_original[nums_feats])"
      ],
      "metadata": {
        "id": "E8pj8CJcOTe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Univariate Analysis\n",
        "\n",
        "## Step 3.1: Numerical variables Analysis"
      ],
      "metadata": {
        "id": "fu8UyBYkeI9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bins\n",
        "bins = [0, 5000, 10000, 15000, loan_original['BorrowerTotalMonthlyIncome'].max()]\n",
        "group = ['Low', 'Average', 'High', 'VeryHigh']\n",
        "loan_original['IncomeBin'] = pd.cut(loan_original['BorrowerTotalMonthlyIncome'], bins, labels=group)\n",
        "\n",
        "# Use seaborn countplot\n",
        "sns.countplot(x='IncomeBin', data=loan_original, hue='Approved')\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nfxoH9FiOTkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count approvals and total per bin\n",
        "acceptance_rate = (\n",
        "    loan_original\n",
        "    .groupby('IncomeBin')\n",
        "    .apply(lambda x: (x['Approved'] == 'Yes').mean())  # or (x['LoanStatus'] == 1).mean()\n",
        "    .reset_index(name='AcceptanceRate')\n",
        ")\n",
        "\n",
        "print(acceptance_rate)\n"
      ],
      "metadata": {
        "id": "21D8LMyZ7Syy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verdict: Our hypothesis that Applicants with high incomes should have more chances of loan approval holds true.\n"
      ],
      "metadata": {
        "id": "sQv9F8d176w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(loan_original['BorrowerAge'], hist = False, kde = True,\n",
        "                 kde_kws = {'shade': True, 'linewidth': 3})"
      ],
      "metadata": {
        "id": "0kbz1PUbOf_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(loan_original['BorrowerYearsInSchool'], hist = False, kde = True,\n",
        "                 kde_kws = {'shade': True, 'linewidth': 3})"
      ],
      "metadata": {
        "id": "UDftxCFzOhmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create bins for Education\n",
        "bins=[0,12,16,18,loan_original.BorrowerYearsInSchool.max()]\n",
        "group=['Higher School','UnderGrad','PostGrad','PHD']\n",
        "loan_original['Education']=pd.cut(loan_original['BorrowerYearsInSchool'],bins,labels=group)\n",
        "sns.countplot(x='Education',data=loan_original,hue='Approved')\n",
        "\n"
      ],
      "metadata": {
        "id": "L4GhhESeOTm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count approvals and total per bin\n",
        "acceptance_rate = (\n",
        "    loan_original\n",
        "    .groupby('Education')\n",
        "    .apply(lambda x: (x['Approved'] == 'Yes').mean())  # or (x['LoanStatus'] == 1).mean()\n",
        "    .reset_index(name='AcceptanceRate')\n",
        ")\n",
        "\n",
        "print(acceptance_rate)\n"
      ],
      "metadata": {
        "id": "AvV1QueH8jhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of records with negative values : {loan_original[loan_original.DTI < 0].shape[0]}\")\n",
        "loan_original = loan_original[loan_original['DTI'] >= 0].copy()\n"
      ],
      "metadata": {
        "id": "rW6r5vS38zKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original['DTI'].describe()"
      ],
      "metadata": {
        "id": "X7ETJk1H9zby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original['DTI'].value_counts(ascending=False)"
      ],
      "metadata": {
        "id": "_iKE7Q7z95MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(loan_original['DTI'].dropna(), bins=30, edgecolor='black')\n",
        "plt.title(\"Distribution of Debt-to-Income Ratio (DTI)\")\n",
        "plt.xlabel(\"DTI (%)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U-9gukYt9vgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSQjclZV9wxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Make fine DTI bins\n",
        "bins = list(range(0, 101, 5))  # 0–5, 5–10, ..., 95–100\n",
        "loan_original['DTI_FineBin'] = pd.cut(loan_original['DTI'], bins=bins, include_lowest=True)\n",
        "\n",
        "# 2) Coerce Approved -> numeric 1/0\n",
        "# Try common patterns; fall back to to_numeric\n",
        "if loan_original['Approved'].dtype == bool:\n",
        "    loan_original['Approved_num'] = loan_original['Approved'].astype(int)\n",
        "else:\n",
        "    # Map common string labels; adjust if your labels differ\n",
        "    mapping = {\n",
        "        'Approved': 1, 'approved': 1, 'Yes': 1, 'Y': 1, '1': 1, 1: 1,\n",
        "        'Rejected': 0, 'rejected': 0, 'No': 0, 'N': 0, '0': 0, 0: 0\n",
        "    }\n",
        "    loan_original['Approved_num'] = (\n",
        "        loan_original['Approved']\n",
        "        .map(mapping)\n",
        "        .pipe(lambda s: pd.to_numeric(s, errors='coerce'))\n",
        "        .astype('float')  # keep float so mean works; you can astype(int) later if needed\n",
        "    )\n",
        "\n",
        "# 3) Compute acceptance rate per fine bin\n",
        "acceptance_by_finebin = (\n",
        "    loan_original\n",
        "    .groupby('DTI_FineBin', observed=True)['Approved_num']\n",
        "    .mean()\n",
        "    .reset_index(name='AcceptanceRate')\n",
        ")\n",
        "\n",
        "# (Optional) also include counts to assess stability\n",
        "counts_by_finebin = (\n",
        "    loan_original\n",
        "    .groupby('DTI_FineBin', observed=True)['Approved_num']\n",
        "    .agg(TotalApps='count', ApprovedApps=lambda s: int(np.nansum(s)))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "print(acceptance_by_finebin.head(20))\n",
        "print(counts_by_finebin.head(20))\n"
      ],
      "metadata": {
        "id": "EkJPSvbSDwy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [0, 30, 50, 80, loan_original['DTI'].max()]\n",
        "labels = ['Low (0-30)', 'Moderate (30-50)', 'High (50-80)', 'Risky (80+)']\n",
        "\n",
        "loan_original['DTI_Bin'] = pd.cut(\n",
        "    loan_original['DTI'], bins=bins, labels=labels, include_lowest=True\n",
        ")\n",
        "\n",
        "acceptance_by_bin = (\n",
        "    loan_original.groupby('DTI_Bin')['Approved_num']\n",
        "    .mean()\n",
        "    .reset_index(name='AcceptanceRate')\n",
        ")\n",
        "\n",
        "print(acceptance_by_bin)\n"
      ],
      "metadata": {
        "id": "G62FNt4bDwwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jCctQUcDwrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "catVars=['LeadSourceGroup','LoanType','LoanPurpose', 'Education','BorrowerOwnRent','ZipCode','LoanOfficer']\n",
        "numVars=['TotalLoanAmount','CreditScore','CLTV', 'DTI','BorrowerAge',\n",
        "       'TotalIncome']"
      ],
      "metadata": {
        "id": "u44Bs1j0fbmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Check distribution of all numerical features\n",
        "plt.figure(figsize=(20,15), facecolor='white')\n",
        "# plot numbering starts at 1, not 0\n",
        "plot_number = 1\n",
        "for  col in numVars:\n",
        "    # Inside of an image that's a 15x13 grid, put this\n",
        "    # graph in the in the plot_number slot.\n",
        "    ax = plt.subplot(4, 3, plot_number)\n",
        "    sns.distplot(loan_original[col], hist = False, kde = True,\n",
        "                 kde_kws = {'shade': True, 'linewidth': 3})\n",
        "    ax.set_title(col)\n",
        "    # Go to the next plot for the next loop\n",
        "    plot_number = plot_number + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "S7ZJ6VKQe54W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  # Optional, for additional styling\n",
        "\n",
        "# Assuming 'loan_data' is your DataFrame and numVars is the list of numerical variables\n",
        "# Replace 'loan_data' and 'numVars' with the actual names in your dataset.\n",
        "\n",
        "# Set the style (optional, for better visualization)\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# List of numerical variables\n",
        "num_vars = ['TotalLoanAmount', 'CreditScore', 'CLTV', 'DTI', 'BorrowerAge', 'TotalIncome']\n",
        "\n",
        "# Create subplots in a 3x3 grid\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
        "\n",
        "# Flatten the 2D array of subplots into a 1D array\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot histograms for each numerical variable\n",
        "for i, var in enumerate(num_vars):\n",
        "    sns.histplot(loan_original[var], bins=30, kde=True, color='skyblue', ax=axes[i])\n",
        "\n",
        "    # Add labels and title\n",
        "    axes[i].set_title(f'Distribution of {var}')\n",
        "    axes[i].set_xlabel(var)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "# Remove empty subplots if there are fewer than 9 variables\n",
        "for j in range(len(num_vars), 9):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ntq2TNJ6HLqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference:-\n",
        "TotalLoanAmount & BorrowerAge have good distribution in comparison with other features.\n",
        "Most of the applicants are in the age of 50.\n",
        "CoBorrowerAge has got bimodal distribution ,one is centered around 0 and another is centered around 50. There seems to some problem with this feature.\n",
        "CoBorrowerYearsInSchool has got bimodal distribution ,one is centered around 0 and another is centered around 14. There seems to some problem with this feature.\n",
        "\n",
        "\n",
        "# Analysis:\n",
        "**A. Total Loan Amount:**\n",
        "  If the distribution of loan amounts is right-skewed, it means that the majority of loan amounts are concentrated on the lower end, and there are fewer instances of higher loan amounts. Here are some inferences you can draw:\n",
        "\n",
        "\n",
        "1. **Most Loans are Small:** The peak of the distribution (mode) is likely to be on the left side, indicating that most loans have relatively smaller amounts. This could suggest that a significant proportion of the loans granted are for lower amounts.\n",
        "\n",
        "2. **Fewer Large Loans:** The right tail of the distribution is longer, indicating that there are fewer instances of larger loan amounts. This might suggest that there are fewer borrowers or instances where substantial loan amounts are requested or approved.\n",
        "\n",
        "3. **Potential Outliers:** Right-skewness often implies the presence of outliers on the higher end. Outliers could be instances where the loan amount is significantly larger than the majority of the loans. These outliers might be worth investigating further, as they could represent unique or special cases.\n",
        "\n",
        "4. **Income Disparities:** A right-skewed distribution might reflect income disparities among the borrowers. It could indicate that a large proportion of borrowers have lower incomes and are requesting smaller loan amounts, while a smaller group of borrowers may have higher incomes and are eligible for larger loan amounts.\n",
        "\n",
        "5. **Risk Assessment:** From a lending perspective, a right-skewed distribution might influence risk assessment. It suggests that the majority of borrowers are seeking smaller loans, which could be less risky. However, the presence of outliers implies that there might be higher-risk loans that need closer scrutiny.\n",
        "\n",
        "In any data analysis, it's important to consider the context of the data and the specific characteristics of the population being studied. Exploring the reasons behind the observed distribution and understanding the implications for decision-making are crucial steps in drawing meaningful inferences.\n",
        "\n",
        "\n",
        "\n",
        "**B. Credit Score:**\n",
        "If the distribution of Credit Scores is left-skewed with a long left tail, several inferences can be made about the data:\n",
        "\n",
        "1. **Majority of Scores are Higher:**\n",
        "   - The peak of the distribution is on the right side, indicating that the majority of credit scores are higher. This could mean that a significant portion of the population has relatively good credit.\n",
        "\n",
        "2. **Fewer Instances of Low Credit Scores:**\n",
        "   - The long left tail suggests that there are fewer instances of very low credit scores. This could indicate that a smaller proportion of individuals in the dataset have poor credit.\n",
        "\n",
        "3. **Potential Outliers on the Low End:**\n",
        "   - The left tail also suggests the presence of outliers or extreme values on the lower end of the credit score spectrum. These outliers could represent individuals with exceptionally low credit scores.\n",
        "\n",
        "4. **Skewed Distribution:**\n",
        "   - The skewness to the left indicates that the distribution is not symmetrical. This might be due to factors such as economic conditions, lending practices, or other external influences that cause a concentration of higher credit scores.\n",
        "\n",
        "5. **Positive Skewness:**\n",
        "   - The left skewness implies a positive skewness, indicating that the mean is likely greater than the median. In other words, the average credit score might be higher than the middle point of the distribution.\n",
        "\n",
        "6. **Potential Need for Transformation:**\n",
        "   - Depending on the analysis and modeling techniques, the left-skewed distribution may benefit from transformation (e.g., logarithmic or square root) to make it more symmetrical and meet the assumptions of certain statistical methods.\n",
        "\n",
        "7. **Creditworthiness of the Population:**\n",
        "   - The distribution suggests that a significant portion of the population has good creditworthiness, but there are still individuals with lower credit scores. This information could be valuable for financial institutions or businesses that use credit scores for decision-making.\n",
        "\n",
        "It's essential to consider the context of the data and the specific goals of the analysis when interpreting the characteristics of the distribution. Additionally, further exploration and analysis may be needed to understand the factors contributing to the observed credit score distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**C. CLTV:**\n",
        "If the distribution of Customer Lifetime Value (CLTV) is left-skewed when visualized using `sns.distplot` (a seaborn function for plotting distributions), it implies that the majority of customers have lower CLTV values, with a tail extending towards higher values. Here are some potential inferences you can draw:\n",
        "\n",
        "1. **Majority of Customers have Lower CLTV:**\n",
        "   - The peak of the distribution is towards the lower end of CLTV values, indicating that most customers have relatively lower lifetime values.\n",
        "\n",
        "2. **Potential Outliers with High CLTV:**\n",
        "   - The left skewness suggests the presence of a few customers with significantly higher CLTV values, appearing as outliers. These customers contribute to the tail on the higher end of the distribution.\n",
        "\n",
        "3. **Varied Customer Segments:**\n",
        "   - The distribution might indicate the existence of distinct segments within the customer base. The majority of customers may fall into one segment with lower CLTV, while a smaller segment may exhibit higher CLTV.\n",
        "\n",
        "4. **Potential for Targeted Marketing:**\n",
        "   - Businesses may consider targeting marketing efforts towards the segment with lower CLTV to improve customer retention and increase their lifetime value.\n",
        "\n",
        "5. **Opportunity for Improvement:**\n",
        "   - If the goal is to increase overall CLTV, businesses may need to explore strategies to enhance customer loyalty, engagement, and satisfaction, particularly within the segment showing lower CLTV.\n",
        "\n",
        "6. **Understanding Customer Behavior:**\n",
        "   - Left-skewed distributions might indicate that customers tend to make a few high-value transactions but are less likely to remain engaged over an extended period.\n",
        "\n",
        "7. **Consideration for Business Strategy:**\n",
        "   - The left-skewed CLTV distribution may influence strategic decisions related to resource allocation, marketing budgeting, and customer relationship management.\n",
        "\n",
        "It's important to note that the interpretation of the distribution should be done in the context of the specific business and its goals. Additionally, examining the underlying factors contributing to the distribution and considering external variables can provide a more comprehensive understanding of customer behavior and CLTV patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**D. DTI:**\n",
        "If the Seaborn `sns.distplot` for Debt-to-Income Ratio (DTI) is right-skewed with a long tail on the right side, several inferences can be made about the distribution of the data. Here are some interpretations:\n",
        "\n",
        "1. **Positive Skewness:**\n",
        "   - The distribution is positively skewed, meaning that the majority of the data points have lower DTI values, but there is a long tail towards higher DTI values.\n",
        "   - This suggests that a significant portion of the population has lower debt-to-income ratios, which is generally considered favorable.\n",
        "\n",
        "2. **Outliers:**\n",
        "   - The long tail on the right side indicates the presence of outliers with higher DTI values. These outliers might represent individuals or cases with exceptionally high debt relative to their income.\n",
        "\n",
        "3. **Potential Financial Risk:**\n",
        "   - High DTI values are often associated with financial risk, as they indicate a larger proportion of income being used to cover debt obligations. The presence of a long tail on the right side may suggest that there is a subset of the population facing higher financial risk.\n",
        "\n",
        "4. **Income Disparities:**\n",
        "   - The skewness in the distribution may also suggest income disparities, with a significant portion of the population having lower incomes and consequently lower DTI values, while a smaller group has higher incomes and higher DTI values.\n",
        "\n",
        "5. **Possible Need for Financial Education or Assistance:**\n",
        "   - The distribution could indicate a need for financial education or assistance for those with higher DTI values, as they may be at a higher risk of financial strain or difficulty in managing their debt.\n",
        "\n",
        "6. **Lending and Credit Risk:**\n",
        "   - For financial institutions, a right-skewed distribution may imply that a majority of their clients have manageable DTI ratios, but there is a smaller subset with higher DTI values that could pose a higher credit risk.\n",
        "\n",
        "7. **Policy Implications:**\n",
        "   - Policymakers might consider addressing income disparities or implementing targeted interventions to assist individuals with high DTI values, especially if there is a substantial portion of the population affected.\n",
        "\n",
        "It's important to note that these inferences are based on the assumption that DTI follows a right-skewed distribution. The interpretation may vary depending on the context of the data and the specific characteristics of the population under consideration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**E. Total Income:**\n",
        "If the distribution of Total Income is right-skewed with a long tail on the right side, there are several inferences you can draw:\n",
        "\n",
        "1. **Positive Skewness:** The fact that the distribution is right-skewed indicates that there is a tail on the right side of the distribution. This implies that there are a few high-income individuals or cases that contribute to the skewness. The majority of the data points are likely clustered towards the lower end of the income scale.\n",
        "\n",
        "2. **Outliers or High-Income Individuals:** The long tail on the right side suggests the presence of outliers or individuals with significantly higher incomes than the rest of the population. These outliers can have a substantial impact on the mean of the distribution, pulling it towards higher values.\n",
        "\n",
        "3. **Income Inequality:** The right-skewed distribution implies income inequality, where a small proportion of the population has much higher incomes than the majority. This could be an indication of a wealth gap within the studied population.\n",
        "\n",
        "4. **Log Transformation Consideration:** In cases where the distribution is highly right-skewed, it might be beneficial to consider log-transforming the data. This can help in reducing the impact of extreme values and make the distribution more symmetric.\n",
        "\n",
        "5. **Median vs. Mean:** Due to the skewness, the mean (average) income may be higher than the median income. The median is a more robust measure of central tendency in skewed distributions as it is less affected by extreme values.\n",
        "\n",
        "6. **Financial Planning Considerations:** Understanding the skewness of income distribution is important for financial planning and policy-making. It helps identify the income levels of the majority of the population as well as the potential challenges associated with high-income individuals.\n",
        "\n",
        "7. **Tail Analysis:** Examining the long tail on the right side can provide insights into the extent of the income disparities. It might be interesting to investigate the specific characteristics of individuals in the high-income bracket and understand the factors contributing to their outlier status.\n",
        "\n",
        "In summary, a right-skewed distribution with a long tail on the right side indicates income inequality, the presence of outliers, and suggests a need for careful analysis when using the mean as a measure of central tendency.\n",
        "\n"
      ],
      "metadata": {
        "id": "D9vVVot0e_z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Filter out extreme outliers\n",
        "\n",
        "loan_original=loan_original[loan_original.CLTV<110]\n",
        "loan_original=loan_original[loan_original.TotalLoanAmount<600000]\n",
        "loan_original=loan_original[loan_original.CreditScore>550]\n",
        "loan_original=loan_original[loan_original.TotalIncome<=30000]"
      ],
      "metadata": {
        "id": "HEWRWLQ9e5xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.2: Categorical variables Analysis"
      ],
      "metadata": {
        "id": "iKT4LmWKPPzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check distribution of all categorical variables\n",
        "sns.set(font_scale=1.5)\n",
        "### Check distribution of all features\n",
        "plt.figure(figsize=(20,30), facecolor='white')\n",
        "# plot numbering starts at 1, not 0\n",
        "plot_number = 1\n",
        "for  col in catVars:\n",
        "    # Inside of an image that's a 15x13 grid, put this\n",
        "    # graph in the in the plot_number slot.\n",
        "    ax = plt.subplot(4, 2, plot_number)\n",
        "    loan_original[col].value_counts(normalize=True).plot(kind='bar')\n",
        "    ax.set_title(col)\n",
        "    # Go to the next plot for the next loop\n",
        "    plot_number = plot_number + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "PElDMnYKPSsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Bivariate Analysis\n",
        "4.1 Categorical Independent Variables vs Target Variable"
      ],
      "metadata": {
        "id": "PusiDTwfSzb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming loan_original is your DataFrame\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size if needed\n",
        "\n",
        "# Create the countplot with tilted x-axis labels\n",
        "sns.countplot(x='Education', data=loan_original, hue='Approved', palette=['#CD6155', '#3388FF'])\n",
        "\n",
        "# Tilt x-axis labels\n",
        "plt.xticks(rotation=45)  # Adjust the rotation angle as needed\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GvMsYpJkSsru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Check distribution of all Categorical features\n",
        "plt.figure(figsize=(20,20), facecolor='white')\n",
        "# plot numbering starts at 1, not 0`\n",
        "plot_number = 1\n",
        "for  col in catVars[:-1]:\n",
        "    # Inside of an image that's a 15x13 grid, put this\n",
        "    # graph in the in the plot_number slot.\n",
        "    ax = plt.subplot(len(catVars[:-1]), 1, plot_number)\n",
        "    sns.countplot(x=col,data=loan_original,hue='Approved',palette=['#CD6155','#3388FF'])\n",
        "    ax.set_title(col)\n",
        "    # Go to the next plot for the next loop\n",
        "    plot_number = plot_number + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "KzgN1ECWSspG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference :-\n",
        "Loan Approval rate is high for leads coming through Internet , TV & Radio channel in comparision with other channels. Internet is contributing highest in it.\n",
        "Loan Approval rate is high for Refinance-Cash-out & Refinance .\n",
        "Loan Approval rate is high for conventional loan types.\n",
        "Loan Approval rate is high for loan applicants ,who've their own home.\n",
        "Loan Approval rate is high for loan applicants,who're are from ZipCode 75 & 76."
      ],
      "metadata": {
        "id": "QA2mtqzfTGwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check loan Approval status based on total income\n",
        "loan_original.groupby('Approved')['BorrowerTotalMonthlyIncome'].mean().plot.bar()\n"
      ],
      "metadata": {
        "id": "Ay0afOdZSsb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Difference in mean income based on Loan Approval status is quite marginal. Let's create bins for applicants's total income and analyze with respect to Loan Approval Status\n"
      ],
      "metadata": {
        "id": "QtgKs1-iTTJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  # Optional, for additional styling\n",
        "\n",
        "# Assuming 'loan_data' is your DataFrame and 'BorrowerTotalMonthlyIncome' is the income variable\n",
        "# Replace 'loan_data' and 'BorrowerTotalMonthlyIncome' with the actual names in your dataset.\n",
        "\n",
        "# Set the style (optional, for better visualization)\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(loan_original['BorrowerTotalMonthlyIncome'], bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Distribution of Borrower Total Monthly Income')\n",
        "plt.xlabel('Borrower Total Monthly Income')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nxTw2dGkGZNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create bins\n",
        "bins=[0,5000,10000,15000,loan_original.BorrowerTotalMonthlyIncome.max()]\n",
        "group=['Low','Average','High','VeryHigh']\n",
        "loan_original['IncomeBin']=pd.cut(loan_original['BorrowerTotalMonthlyIncome'],bins,labels=group)\n",
        "sns.countplot(x='IncomeBin',data=loan_original,hue='Approved',palette=['#CD6155','#3388FF'])"
      ],
      "metadata": {
        "id": "rYpg2f2TTWo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that It supports out hypothesis that applicants with higher income would have higher chance of loan being approved\n"
      ],
      "metadata": {
        "id": "bkr0R7u9TbS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We will analyze the co-applicant income\n",
        "#Check loan Approval status based on total income\n",
        "loan_original.groupby('Approved')['CoBorrowerTotalMonthlyIncome'].mean().plot.bar()"
      ],
      "metadata": {
        "id": "vqVn91ThTWfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's significant difference between mean income of coapplicants with respect to Loan Approval status. We've already seen that CoBorrowerTotalMonthlyIncome hass bimodal distribution\n"
      ],
      "metadata": {
        "id": "ysb_sI1GTfsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create bins\n",
        "bins=[0,5000,10000,15000,loan_original.CoBorrowerTotalMonthlyIncome.max()]\n",
        "group=['Low','Average','High','VeryHigh']\n",
        "loan_original['IncomeBin']=pd.cut(loan_original['CoBorrowerTotalMonthlyIncome'],bins,labels=group)\n",
        "sns.countplot(x='IncomeBin',data=loan_original,hue='Approved',palette=['#CD6155','#3388FF'])"
      ],
      "metadata": {
        "id": "_bUZJDyETWOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that It supports out hypothesis that applicants with higher income would have higher chance of loan being approved. Let's combine both applicant's income and analyze\n"
      ],
      "metadata": {
        "id": "XP3N4eVITjsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check loan Approval status based on total income\n",
        "loan_original.groupby('Approved')['TotalIncome'].mean().plot.bar()"
      ],
      "metadata": {
        "id": "b9bvltOWTXmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that proportion of loans getting approved for applicants having low Total_Income is very less compared to that of applicants with Average, High & Very High Income.\n"
      ],
      "metadata": {
        "id": "qjFdMW1zTnPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let’s visualize the Loan Amount variable.\n",
        "loan_original.groupby('Approved')['TotalLoanAmount'].mean().plot.bar()\n"
      ],
      "metadata": {
        "id": "DqeZdXkpTqHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "~ ( It can be seen that the proportion of approved loans is higher for Low and Average Loan Amount as compared to that of High Loan Amount which supports our hypothesis in which we considered that the chances of loan approval will be high when the loan amount is less.)\n",
        "Let’s drop the bins which we created for the exploration part.\n",
        "We will also convert the target variable’s categories into 0 and 1 so that we can find its correlation with numerical variables. One more reason to do so is few models like logistic regression takes only numeric values as input. We will replace N with 0 and Y with 1.\n"
      ],
      "metadata": {
        "id": "z3trWfV8Tr_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Box plot by Target Variable\n",
        "boxplot = loan_original.boxplot(column=numVars,by='Approved',figsize=(15,20) ,layout=(6, 2),grid=True, rot=45, fontsize=15)\n"
      ],
      "metadata": {
        "id": "xyjQxFiOSsQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original.Approved.replace({'Yes':1,'No':0},inplace=True)\n",
        "\n",
        "# 5. Correlation Matrix\n",
        "\n",
        "numVars+=['Approved']\n",
        "\n",
        "#Correlation Plot\n",
        "f, ax = plt.subplots(figsize=(11, 15))\n",
        "# clms=['FLCount','Internet','Radio','TV','SocialMedia','Others']\n",
        "corr_matrix = loan_original[numVars].corr()\n",
        "heatmap = sns.heatmap(corr_matrix,\n",
        "#                       mask = mask,\n",
        "                      square = True,\n",
        "                      linewidths = .5,\n",
        "                      cmap = 'coolwarm',\n",
        "                      cbar_kws = {'shrink': .4,\n",
        "                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n",
        "                      vmin = -1,\n",
        "                      vmax = 1,\n",
        "                      annot = True,\n",
        "                      annot_kws = {'size': 12})\n",
        "\n",
        "#add the column names as labels\n",
        "ax.set_yticklabels(corr_matrix.columns, rotation = 0)\n",
        "ax.set_xticklabels(corr_matrix.columns)\n",
        "\n",
        "sns.set_style({'xtick.bottom': True}, {'ytick.left': True})"
      ],
      "metadata": {
        "id": "lLFa3WI7XhQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Split the data into train and test set\n",
        "Note: Apply all your data preprocessing steps in train set first and keep test set aside."
      ],
      "metadata": {
        "id": "v1glHAIYJd5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_bkp = loan_original.copy(deep=True)"
      ],
      "metadata": {
        "id": "fqSIl8UIo8A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## Split Features and Target Varible ############\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'loan_original' is your DataFrame\n",
        "# Replace 'loan_original' with the actual name of your DataFrame\n",
        "\n",
        "# Split original data into train_df and test_df\n",
        "train_df, test_df = train_test_split(loan_original, test_size=0.2, stratify=loan_original['Approved'], random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L2tT7TOAJcvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Feature Engineering"
      ],
      "metadata": {
        "id": "y0yrfR94KExZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.1: Missing Value Imputation"
      ],
      "metadata": {
        "id": "Pcjq0CvAkJ43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_original.isna().sum()\n"
      ],
      "metadata": {
        "id": "kapriYr8YkWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical values imputation tips :\n",
        "Find the number of NaN entries in the age column and their row index. Calculate the mean, Standard deviation of the Age column and check the distribution of the age column.We can fill the missing values with randomly generated integer values between (mean+Standard deviation, mean-Standard deviation). Use : np.isnan; np.random.randint; concept of slicing dataframe. Convert the age column as an integer data type.\n",
        "\n"
      ],
      "metadata": {
        "id": "69Yug-4oKHHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_avg = train_df['BorrowerAge'].mean() # Average Age\n",
        "print(\"Mean of Age :\", age_avg)\n",
        "age_std = train_df['BorrowerAge'].std() # Standard Deviation\n",
        "print(\"Std of Age :\",age_std)\n",
        "age_null_count = train_df['BorrowerAge'].isnull().sum() # Number of null records in this feature\n",
        "print(\"Count of Nan in Age column :\",age_null_count)\n",
        "## Plotting the histogram plot\n",
        "train_df['BorrowerAge'].plot(kind='hist',bins=8,edgecolor='r')"
      ],
      "metadata": {
        "id": "ofFBOhhMKCwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function, this takes dataframe as a parameter and returns the transformed\n",
        "# Dataframe, filling the missing values of age feature as discussed above.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to impute missing values for all numerical variables\n",
        "def impute_numerical(data_frame, numerical_vars):\n",
        "    df = data_frame.copy()\n",
        "    np.random.seed(0)\n",
        "\n",
        "    for var in numerical_vars:\n",
        "        # Calculate mean and standard deviation for imputation\n",
        "        mean_value = df[var].mean()\n",
        "        std_value = df[var].std()\n",
        "\n",
        "        # Impute missing values using a random distribution\n",
        "        null_count = df[var].isnull().sum()\n",
        "        null_random_list = np.random.randint(mean_value - std_value, mean_value + std_value, size=null_count)\n",
        "        df.loc[np.isnan(df[var]), var] = null_random_list\n",
        "        df[var] = df[var].astype(df[var].dtype)  # Ensure the correct data type\n",
        "\n",
        "    return df\n",
        "\n",
        "# Assuming numerical_vars is a list of numerical variables\n",
        "numerical_vars = ['BorrowerAge', 'TotalLoanAmount', 'CreditScore', 'CLTV', 'DTI', 'BorrowerTotalMonthlyIncome']\n",
        "\n",
        "# Call the function to impute missing values for numerical variables\n",
        "train_df = impute_numerical(train_df, numerical_vars)\n",
        "test_df = impute_numerical(test_df, numerical_vars)\n",
        "\n",
        "# Check the information after imputation\n",
        "train_df[numerical_vars].isna().sum()\n"
      ],
      "metadata": {
        "id": "6k8uI3F9L7Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the information after imputation\n",
        "test_df[numerical_vars].isna().sum()"
      ],
      "metadata": {
        "id": "75d4iztDiDda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_missing_values(X, features_to_impute):\n",
        "    for feature in features_to_impute:\n",
        "        mode_value = X[feature].mode().values[0]\n",
        "        X[feature].fillna(mode_value, inplace=True)\n",
        "\n",
        "    return X\n",
        "\n",
        "# Specify the features to impute\n",
        "cat_vars = ['ZipCode', 'BorrowerOwnRent', 'BorrowerYearsInSchool', 'Education','LeadSourceGroup']\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X_train is your training data and X_test is your test data\n",
        "train_df = impute_missing_values(train_df, cat_vars)\n",
        "test_df = impute_missing_values(test_df, cat_vars)\n"
      ],
      "metadata": {
        "id": "b-SGwQaRrAAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[cat_vars].isna().sum()"
      ],
      "metadata": {
        "id": "x_d5VpwuNbIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[cat_vars].isna().sum()"
      ],
      "metadata": {
        "id": "RZeBJ_7nOMS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMZDLVCZbYuz"
      },
      "source": [
        "## Step 6.2: Outliers Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZTaEPO4beUz"
      },
      "outputs": [],
      "source": [
        "#outliers check\n",
        "def identifyOutliers(df,clmn):\n",
        "    print('Outliers check for feature :',clmn)\n",
        "    #Outliers check\n",
        "    Q1 = df[clmn].quantile(0.25)\n",
        "    Q3 = df[clmn].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    Lower_Whisker = Q1-(1.5*IQR)\n",
        "    Upper_Whisker = Q3+(1.5*IQR)\n",
        "    print(\"Q1:\",Q1,\"Q3:\",Q3,\"IQR:\",IQR,\"Lower_Whisker:\",Lower_Whisker,\"Upper_Whisker:\",Upper_Whisker,'loan_original Range :',(Upper_Whisker-Lower_Whisker))\n",
        "    #Identify Outliers\n",
        "    OutliersSize=df[(df[clmn] < (Lower_Whisker)) |(df[clmn] > (Upper_Whisker))].shape[0]\n",
        "    loan_originalSize=df.shape[0]\n",
        "    print('% of outliers :',(round((OutliersSize/loan_originalSize)*100,2)),'\\n')\n",
        "    return Lower_Whisker,Upper_Whisker\n",
        "\n",
        "def treatOutliers(df,clmn):\n",
        "    Lower_Whisker,Upper_Whisker=identifyOutliers(df,clmn)\n",
        "    #Treat outliers with mean values\n",
        "    meanValue=round(df[clmn].mean(),2)\n",
        "    print('meanValue for :',clmn,meanValue)\n",
        "    lst=[meanValue if((entry<Lower_Whisker) | (entry>Upper_Whisker)) else entry for entry in df[clmn] ]\n",
        "    return lst\n",
        "\n",
        "\n",
        "for col in train_df[numVars[:-1]]:\n",
        "    identifyOutliers(train_df,col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShS5Eq3JbhFc"
      },
      "outputs": [],
      "source": [
        "# Outliers check\n",
        "train_df.plot(kind='box',subplots=True,layout=(4,3),sharex=False,sharey=False,figsize=(15,10),\n",
        "       title='Box plot for each input variable')\n",
        "plt.savefig('BoxPlots')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lneFg5nJbiyr"
      },
      "outputs": [],
      "source": [
        "for col in train_df[numVars[:-1]]:\n",
        "    identifyOutliers(train_df,col)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijMRNIhnVVWo"
      },
      "outputs": [],
      "source": [
        "# Outliers check\n",
        "train_df.plot(kind='box',subplots=True,layout=(4,3),sharex=False,sharey=False,figsize=(15,15),\n",
        "       title='Box plot for each input variable')\n",
        "plt.savefig('BoxPlots')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in test_df[numVars[:-1]]:\n",
        "    identifyOutliers(test_df,col)"
      ],
      "metadata": {
        "id": "7YHBg0tor3Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.3: Chi-Square test for categorical variables"
      ],
      "metadata": {
        "id": "NCZXKfxjzzXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Assuming 'loan_data' is your DataFrame\n",
        "# Replace 'loan_data' with the actual name of your DataFrame\n",
        "\n",
        "# Categorical variables\n",
        "categorical_vars = ['LeadSourceGroup', 'ZipCode', 'Education', 'LoanPurpose', 'LoanType', 'BorrowerOwnRent', 'IsCoBorrowerower', 'Approved']\n",
        "\n",
        "# Perform Chi-square tests for each pair of categorical variables\n",
        "for var1 in categorical_vars:\n",
        "    for var2 in categorical_vars:\n",
        "        if var1 != var2:  # Exclude comparisons with the same variable\n",
        "            contingency_table = pd.crosstab(train_df[var1], train_df[var2])\n",
        "\n",
        "            # Perform Chi-square test\n",
        "            chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
        "\n",
        "            # Print results\n",
        "            print(f\"Chi-square test between {var1} and {var2}:\")\n",
        "            print(f\"Chi2 Statistic: {chi2_stat}\")\n",
        "            print(f\"P-Value: {p_value}\")\n",
        "            print(\"\")\n",
        "\n",
        "# Note: This code assumes 'loan_data' is your DataFrame. Replace 'loan_data' with the actual name in your dataset.\n"
      ],
      "metadata": {
        "id": "7DZniUIbFc73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chi-Square result:"
      ],
      "metadata": {
        "id": "HgwlJSRY06YW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-square test results indicate the association between different categorical variables in your loan approval classification use case. Let's analyze the insights and arrange the variables in order of their importance based on the Chi-square test results:\n",
        "\n",
        "1. **LoanPurpose:**\n",
        "   - Chi2 Statistic: 475.92\n",
        "   - P-Value: 1.65e-105\n",
        "   - **Insight:** The purpose for which the loan is sought (LoanPurpose) has the highest importance and a highly significant association with loan approval. It is a critical factor in predicting loan approval decisions.\n",
        "\n",
        "2. **BorrowerOwnRent:**\n",
        "   - Chi2 Statistic: 229.78\n",
        "   - P-Value: 6.65e-52\n",
        "   - **Insight:** Whether the borrower owns or rents their residence (BorrowerOwnRent) is the second most important variable, showing a strong association with loan approval.\n",
        "\n",
        "3. **ZipCode:**\n",
        "   - Chi2 Statistic: 113.26\n",
        "   - P-Value: 8.37e-23\n",
        "   - **Insight:** The geographical location, represented by the Zip Code, is the third most important variable, indicating a significant association with loan approval.\n",
        "\n",
        "4. **LeadSourceGroup:**\n",
        "   - Chi2 Statistic: 73.15\n",
        "   - P-Value: 4.91e-15\n",
        "   - **Insight:** The source of the loan application (LeadSourceGroup) is moderately important, with a significant association with loan approval.\n",
        "\n",
        "5. **LoanType:**\n",
        "   - Chi2 Statistic: 46.91\n",
        "   - P-Value: 3.64e-10\n",
        "   - **Insight:** The type of loan is another significant variable, although it is less important than LoanPurpose, BorrowerOwnRent, ZipCode, and LeadSourceGroup.\n",
        "\n",
        "6. **Education:**\n",
        "   - Chi2 Statistic: 11.03\n",
        "   - P-Value: 0.0116\n",
        "   - **Insight:** Education level has a statistically significant but relatively smaller impact on loan approval compared to other variables.\n",
        "\n",
        "7. **IsCoBorrower:**\n",
        "   - Chi2 Statistic: 0.0\n",
        "   - P-Value: 1.0\n",
        "   - **Insight:** The variable \"IsCoBorrower\" does not show a significant association with any other variable in the provided results.\n",
        "\n",
        "**Final Verdict:**\n",
        "- Focus on LoanPurpose, BorrowerOwnRent, ZipCode, LeadSourceGroup, and LoanType as they have demonstrated significant associations with loan approval.\n",
        "- Consider the order of importance when making decisions or building models: LoanPurpose is the most crucial, followed by BorrowerOwnRent, ZipCode, LeadSourceGroup, LoanType, Education, and IsCoBorrower.\n",
        "\n",
        "It's important to note that these insights are based on statistical associations, and the practical significance of these variables should also be considered in the context of your specific loan approval model or decision-making process."
      ],
      "metadata": {
        "id": "GhaOUyuC02sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isna().sum()"
      ],
      "metadata": {
        "id": "G1XOL1mQL9d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.4: ANOVA test for numerical variables"
      ],
      "metadata": {
        "id": "HU-mscFkz7Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Assuming 'loan_data' is your DataFrame\n",
        "# Replace 'loan_data' with the actual name of your DataFrame\n",
        "\n",
        "# Categorical variables\n",
        "categorical_vars = ['LeadSourceGroup', 'LoanOfficer', 'Group', 'LoanPurpose', 'LoanType', 'BorrowerOwnRent', 'BorrowerCity', 'BorrowerState', 'Approved']\n",
        "\n",
        "# Numerical variables\n",
        "numerical_vars = ['TotalLoanAmount', 'CreditScore', 'CLTV', 'DTI', 'BorrowerAge', 'BorrowerYearsInSchool', 'BorrowerTotalMonthlyIncome', 'CoBorrowerYearsInSchool', 'CoBorrowerAge', 'CoBorrowerTotalMonthlyIncome']\n",
        "\n",
        "# Perform ANOVA for each numerical variable with 'Approved' as the grouping variable\n",
        "for num_var in numerical_vars:\n",
        "    groups = []\n",
        "    for category in train_df['Approved'].unique():\n",
        "        groups.append(train_df[train_df['Approved'] == category][num_var])\n",
        "\n",
        "    # Perform ANOVA\n",
        "    f_statistic, p_value = f_oneway(*groups)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"ANOVA for {num_var} and Approved:\")\n",
        "    print(f\"F-Statistic: {f_statistic}\")\n",
        "    print(f\"P-Value: {p_value}\")\n",
        "    print(\"\")\n",
        "\n",
        "# Note: This code assumes 'loan_data' is your DataFrame and 'Approved' is the target variable.\n",
        "# Replace 'loan_data' and 'Approved' with the actual names in your dataset.\n"
      ],
      "metadata": {
        "id": "4cEOTHzNFc2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANOVA Test result:\n",
        "\n",
        "The ANOVA test results provide insights into the association between numerical variables and loan approval in your classification use case. Let's analyze the results and arrange the variables in order of their importance based on the F-statistic and p-value:\n",
        "\n",
        "Certainly! Based on the ANOVA test results for numerical variables in your loan approval classification use case, here are the insights and a final arrangement of variables in order of their importance:\n",
        "\n",
        "1. **CLTV (Combined Loan-to-Value):**\n",
        "   - F-Statistic: 166.89\n",
        "   - P-Value: 6.90e-38\n",
        "   - **Insight:** CLTV has the highest importance, with a highly significant association with loan approval. It is a critical factor in predicting loan approval decisions.\n",
        "\n",
        "Note: $$ \\text{CLTV} = \\frac{\\text{Sum of All Loan Amounts}}{\\text{Appraised Property Value}} \\times 100 $$\n",
        "2. **BorrowerYearsInSchool:**\n",
        "   - F-Statistic: 112.45\n",
        "   - P-Value: 3.85e-26\n",
        "   - **Insight:** The number of years the borrower spent in school is the second most important variable, showing a strong association with loan approval.\n",
        "\n",
        "3. **BorrowerAge:**\n",
        "   - F-Statistic: 84.76\n",
        "   - P-Value: 4.01e-20\n",
        "   - **Insight:** Borrower's age is the third most important variable, indicating a significant association with loan approval.\n",
        "\n",
        "4. **TotalLoanAmount:**\n",
        "   - F-Statistic: 39.47\n",
        "   - P-Value: 3.47e-10\n",
        "   - **Insight:** The total loan amount is a significant variable, although it is less important than CLTV, BorrowerYearsInSchool, and BorrowerAge.\n",
        "\n",
        "5. **CoBorrowerTotalMonthlyIncome:**\n",
        "   - F-Statistic: 15.13\n",
        "   - P-Value: 0.00010\n",
        "   - **Insight:** The total monthly income of the co-borrower is another significant variable, indicating a moderate association with loan approval.\n",
        "\n",
        "6. **DTI (Debt-to-Income Ratio):**\n",
        "   - F-Statistic: 10.28\n",
        "   - P-Value: 0.00135\n",
        "   - **Insight:** Debt-to-Income Ratio is a moderately important variable, showing a significant association with loan approval.\n",
        "Note:\n",
        "The formula for Debt-to-Income (DTI) ratio is:\n",
        "\n",
        "$$\\ \\text{DTI} = \\frac{\\text{Total Monthly Debt Payments}}{\\text{Gross Monthly Income}} \\times 100 \\$$\n",
        "\n",
        "7. **CreditScore:**\n",
        "   - F-Statistic: 5.92\n",
        "   - P-Value: 0.01496\n",
        "   -\n",
        "\n",
        "**Insight:** Credit score is a significant variable, although it has a comparatively smaller impact on loan approval compared to other variables.\n",
        "\n",
        "8. **CoBorrowerYearsInSchool:**\n",
        "   - F-Statistic: 29.82\n",
        "   - P-Value: 4.85e-08\n",
        "   - **Insight:** The number of years the co-borrower spent in school is another moderately important variable, indicating a significant association with loan approval.\n",
        "\n",
        "9. **CoBorrowerAge:**\n",
        "   - F-Statistic: 5.06\n",
        "   - P-Value: 0.02457\n",
        "   - **Insight:** Co-borrower's age is a variable with a smaller impact on loan approval compared to other factors.\n",
        "\n",
        "10. **BorrowerTotalMonthlyIncome:**\n",
        "    - F-Statistic: 6.60\n",
        "    - P-Value: 0.01019\n",
        "    - **Insight:** Borrower's total monthly income is the least important among the numerical variables considered, but it still shows a significant association with loan approval.\n",
        "\n",
        "**Final Verdict:**\n",
        "- Focus on CLTV, BorrowerYearsInSchool, and BorrowerAge as they have demonstrated the highest importance and highly significant associations with loan approval.\n",
        "- Consider TotalLoanAmount, CoBorrowerTotalMonthlyIncome, and DTI as they are also significant variables in predicting loan approval.\n",
        "- CreditScore, CoBorrowerYearsInSchool, CoBorrowerAge, and BorrowerTotalMonthlyIncome have smaller impacts on loan approval but are still statistically significant.\n",
        "\n",
        "These insights are based on the statistical associations observed in the ANOVA test, and practical considerations should also be taken into account when making decisions in your loan approval model or decision-making process."
      ],
      "metadata": {
        "id": "8ZMuJOHS1h9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.5: Drop irrelevant variables"
      ],
      "metadata": {
        "id": "TiLhgDYz0D3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop below features\n",
        "#Drop irrelevant fetures\n",
        "unused_vars = ['LeadID','DateAdded','LoanOfficer','Group','IsCoBorrowerower','BorrowerTotalMonthlyIncome',\n",
        "                    'CoBorrowerTotalMonthlyIncome','BorrowerYearsInSchool','CoBorrowerYearsInSchool','CoBorrowerAge','BorrowerCity',\n",
        "                    'BorrowerState','Group','IncomeBin','ApprovalDate']\n",
        "train_df.drop(unused_vars, axis=1, inplace=True)\n",
        "\n",
        "test_df.drop(unused_vars, axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "9gxo4WWSnCAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.6: Split train and test data into X, and Y variables training and testing respectively."
      ],
      "metadata": {
        "id": "YxSJT1Qf0Jw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuWFIxYJkQ0l"
      },
      "outputs": [],
      "source": [
        "# Split train_df into features (X_train) and target variable (y_train)\n",
        "X_train = train_df.drop(columns='Approved')\n",
        "y_train = train_df['Approved']\n",
        "\n",
        "# Split test_df into features (X_test) and target variable (y_test)\n",
        "X_test = test_df.drop(columns='Approved')\n",
        "y_test = test_df['Approved']\n",
        "\n",
        "# Optionally, convert to DataFrames if needed\n",
        "X_train = pd.DataFrame(data=X_train, columns=X_train.columns)\n",
        "X_test = pd.DataFrame(data=X_test, columns=X_test.columns)\n",
        "y_train = pd.DataFrame(data=y_train, columns=['Approved'])\n",
        "y_test = pd.DataFrame(data=y_test, columns=['Approved'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLxz8duUk37a"
      },
      "outputs": [],
      "source": [
        "# # Apply SMOTE to the training data\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrLRMRnZwNkl"
      },
      "outputs": [],
      "source": [
        "catVars.remove('LoanOfficer')\n",
        "catVars"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.7: Encode categorical variables"
      ],
      "metadata": {
        "id": "XdCqp275hFmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifcx81PxNniY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def encode(df, catVars):\n",
        "    if len(catVars) == 0:\n",
        "        return df, None\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "    OHE = encoder.fit(df[catVars])\n",
        "    cat_feature_names = OHE.get_feature_names_out(catVars)\n",
        "\n",
        "    # Prefix encoded values with column names\n",
        "    OHEdf = pd.DataFrame(OHE.transform(df[catVars]), columns=cat_feature_names)\n",
        "\n",
        "    NumDf = df.drop(catVars, axis=1).reset_index(drop=True)\n",
        "\n",
        "    df_ = pd.concat([OHEdf, NumDf], axis=1)\n",
        "\n",
        "    return df_, OHE\n",
        "\n",
        "# Assuming loan_original is your DataFrame\n",
        "X_train, OHE = encode(X_train, catVars)\n",
        "X_test, OHE = encode(X_test, catVars)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YRBHiELSQZ0"
      },
      "outputs": [],
      "source": [
        "X_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqQv3e3NppH5"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjsUXFTBxOuO"
      },
      "outputs": [],
      "source": [
        "X_train.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw4Q8oVicB7B"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(OHE, open('./encoder.pkl' ,'wb'))\n",
        "enc = pickle.load(open('./encoder.pkl','rb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.8: Apply Standard Scalar"
      ],
      "metadata": {
        "id": "o5cojZKdaFz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_train_scaled[0,:]"
      ],
      "metadata": {
        "id": "xLQz3IzhaFD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled[0,:]"
      ],
      "metadata": {
        "id": "q1RE2fbraXCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7 : Model Building\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5wnsAe-NalrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7.1: GridSearch for hyper parameters tuning\n"
      ],
      "metadata": {
        "id": "SxScGVVAkW90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLYJYmuorezG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix ,classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import scikitplot as skplt\n",
        "\n",
        "# Write a function for Grid Search\n",
        "def GridSearch(model,param_grid,cv,X,y,scoring=None):\n",
        "    # Random search of parameters\n",
        "    gcv = GridSearchCV(estimator = model, param_grid=param_grid , refit = True, cv = cv, verbose=1,  n_jobs = -1)\n",
        "    # Fit the model\n",
        "    gcv.fit(X,y)\n",
        "\n",
        "    return gcv.best_params_\n",
        "\n",
        "def modelPerformance(model,X,y,dataType):\n",
        "    print(\"=== Model Performance report on \",dataType,\" Data ===\")\n",
        "    y_pred = model.predict(X)\n",
        "    print(\"=== Confusion Matrix ===\")\n",
        "    print(confusion_matrix(y, y_pred))\n",
        "    print('\\n')\n",
        "    print(\"=== Classification Report ===\")\n",
        "    print(classification_report(y, y_pred))\n",
        "    skplt.metrics.plot_confusion_matrix(y,y_pred, normalize=True, figsize=(8,3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bkp, X_test_bkp = X_train.copy(deep=True), X_test.copy(deep=True)\n",
        "X_train, X_test = X_train_scaled.copy(), X_test_scaled.copy()"
      ],
      "metadata": {
        "id": "KMd07nYHtMbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QviFN1_re2x"
      },
      "outputs": [],
      "source": [
        "# random forest model creation\n",
        "\n",
        "#Model performance on CV Training Data\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix ,classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "param_grid = {\n",
        "              'max_depth': [2, 3,4 ,5],\n",
        "              'min_samples_leaf': [1,3, 5, 10,],\n",
        "              'min_samples_split':[2,3,4],\n",
        "              'max_features':['auto','sqrt'],\n",
        "              'n_estimators':[100,200,300,400]\n",
        "              }\n",
        "\n",
        "\n",
        "# random forest model creation\n",
        "rfc_org = RandomForestClassifier(random_state=123, n_jobs=-1)\n",
        "\n",
        "best_params=GridSearch(rfc_org,param_grid,3,X_train,y_train)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkWJvpJFre5o"
      },
      "outputs": [],
      "source": [
        "rfc_org = RandomForestClassifier(\n",
        "                             max_depth=best_params['max_depth'],\n",
        "                             max_features=best_params['max_features'],\n",
        "                             min_samples_split=best_params['min_samples_split'],\n",
        "                             min_samples_leaf=best_params['min_samples_leaf'],\n",
        "                             n_estimators=best_params['n_estimators'],\n",
        "                             n_jobs=-1\n",
        "\n",
        "                            )\n",
        "rfc_org.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foNi9oYHre8g"
      },
      "outputs": [],
      "source": [
        "#Model performance on Training Data\n",
        "modelPerformance(rfc_org,X_train,y_train,'Train')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKDe6n5qre_Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Model performance on Test Data\n",
        "modelPerformance(rfc_org,X_test,y_test,'Test')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r813mtEmtOpR"
      },
      "outputs": [],
      "source": [
        "###################### SMOTE Sampling ##################################\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "oversampled = SMOTE(random_state=0)\n",
        "X_train_SMOTE, y_train_SMOTE = oversampled.fit_resample(X_train, y_train)\n",
        "y_train_SMOTE.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0kIe1EL0KOj"
      },
      "outputs": [],
      "source": [
        "# random forest model creation\n",
        "rfc_SMOTE = RandomForestClassifier(random_state=123, n_jobs=-1)\n",
        "best_params=GridSearch(rfc_SMOTE,param_grid,3,X_train_SMOTE,y_train_SMOTE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlpThNI00X2E"
      },
      "outputs": [],
      "source": [
        "rfc_SMOTE = RandomForestClassifier(\n",
        "                             max_depth=best_params['max_depth'],\n",
        "                             max_features=best_params['max_features'],\n",
        "                             min_samples_split=best_params['min_samples_split'],\n",
        "                             min_samples_leaf=best_params['min_samples_leaf'],\n",
        "                             n_estimators=best_params['n_estimators'],\n",
        "                             n_jobs=-1\n",
        "\n",
        "                            )\n",
        "rfc_SMOTE.fit(X_train_SMOTE,y_train_SMOTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STj7RG4-smDt"
      },
      "outputs": [],
      "source": [
        "############################ Class_weights ############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtSr7OR_2M8b"
      },
      "outputs": [],
      "source": [
        "#Setting the range for class weights\n",
        "weights = np.linspace(0.0,0.99,200)\n",
        "\n",
        "#Creating a dictionary grid for grid search\n",
        "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO165vL-2NM_"
      },
      "outputs": [],
      "source": [
        "#Fitting grid search to the train data with 5 folds\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# random forest model creation\n",
        "rfc_class_wghts = RandomForestClassifier(random_state=123, n_jobs=-1)\n",
        "\n",
        "gridsearch = GridSearchCV(estimator= rfc_class_wghts,\n",
        "                          param_grid= param_grid,\n",
        "                          cv=StratifiedKFold(n_splits=5, shuffle=True,random_state=42),\n",
        "                          n_jobs=-1,\n",
        "                          scoring='f1',\n",
        "                          verbose=2).fit(X_train, y_train)\n",
        "\n",
        "#Ploting the score for different values of weight\n",
        "# Create a DataFrame for plotting\n",
        "weigh_data = pd.DataFrame({'score': gridsearch.cv_results_['mean_test_score'], 'weight': (1 - weights)})\n",
        "\n",
        "# Create an interactive line plot\n",
        "fig = px.line(weigh_data, x='weight', y='score',\n",
        "              labels={'weight': 'Weight for class 1', 'score': 'F1 score'},\n",
        "              title='Scoring for different class weights',\n",
        "              width=800, height=500)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFR-6QUVccnl"
      },
      "outputs": [],
      "source": [
        "# Display the interactive plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0tWTcxemiuh"
      },
      "outputs": [],
      "source": [
        "rfc_class_wghts = RandomForestClassifier(\n",
        "                             max_depth=best_params['max_depth'],\n",
        "                             max_features=best_params['max_features'],\n",
        "                             min_samples_split=best_params['min_samples_split'],\n",
        "                             min_samples_leaf=best_params['min_samples_leaf'],\n",
        "                             n_estimators=best_params['n_estimators'],\n",
        "                             class_weight={0: 0.75, 1: 0.25},\n",
        "                             n_jobs=-1\n",
        "\n",
        "                            )\n",
        "rfc_class_wghts.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H66bq3DzJ0CL"
      },
      "outputs": [],
      "source": [
        "# Assuming y_train is a DataFrame with \"Approved\" column\n",
        "y_train_array = y_train[\"Approved\"].values.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################## Vanila XgBoost Classifier ##############################\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_child_weight': [1, 3, 5, 10],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3, 0.4]\n",
        "}\n",
        "\n",
        "# XGBoost model creation\n",
        "xgb_org = XGBClassifier(random_state=123, n_jobs=-1)\n",
        "\n",
        "best_params = GridSearch(xgb_org, param_grid, 3, X_train, y_train_array)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Create XGBoost model with the best hyperparameters and class weights\n",
        "xgb_org = XGBClassifier(\n",
        "    max_depth=best_params['max_depth'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    min_child_weight=best_params['min_child_weight'],\n",
        "    subsample=best_params['subsample'],\n",
        "    colsample_bytree=best_params['colsample_bytree'],\n",
        "    gamma=best_params['gamma'],\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_org.fit(X_train, y_train_array)\n"
      ],
      "metadata": {
        "id": "IvzoxMbAZfoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################### SMOTE Sampling ##################################\n",
        "\n",
        "# Assuming y_train is a DataFrame with \"Approved\" column\n",
        "y_train_SMOTE = y_train_SMOTE[\"Approved\"].values.astype(int)\n"
      ],
      "metadata": {
        "id": "P4BeFlG6aXvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################## XgBoost Classifier with SMOTE sampled data ##############################\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_child_weight': [1, 3, 5, 10],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3, 0.4]\n",
        "}\n",
        "\n",
        "# XGBoost model creation\n",
        "xgb_SMOTE = XGBClassifier(random_state=123, n_jobs=-1)\n",
        "\n",
        "best_params = GridSearch(xgb_SMOTE, param_grid, 3, X_train_SMOTE, y_train_SMOTE)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Create XGBoost model with the best hyperparameters and class weights\n",
        "xgb_SMOTE = XGBClassifier(\n",
        "    max_depth=best_params['max_depth'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    min_child_weight=best_params['min_child_weight'],\n",
        "    subsample=best_params['subsample'],\n",
        "    colsample_bytree=best_params['colsample_bytree'],\n",
        "    gamma=best_params['gamma'],\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_SMOTE.fit(X_train_SMOTE, y_train_array)\n"
      ],
      "metadata": {
        "id": "c4u5fwd4ZfkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfXWbNMbZff0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming y_train is a DataFrame with \"Approved\" column\n",
        "y_train_array = y_train[\"Approved\"].values.astype(int)"
      ],
      "metadata": {
        "id": "eeBx5Ww9ZfaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19rsvzQp8eek"
      },
      "outputs": [],
      "source": [
        "# ############################## XgBoost Classifier with class weights ##############################\n",
        "# import plotly.express as px\n",
        "# from xgboost import XGBClassifier\n",
        "# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# # Assuming y_train is a DataFrame with \"Approved\" column\n",
        "# # y_train_array = y_train[\"Approved\"].astype('category').cat.codes\n",
        "\n",
        "# # XGBoost model creation\n",
        "# xgb_class_wghts = XGBClassifier(random_state=123, n_jobs=-1, objective='binary:logistic', eval_metric='logloss')\n",
        "\n",
        "\n",
        "\n",
        "# # Fitting grid search to the train data with 5 folds\n",
        "# gridsearch = GridSearchCV(estimator=xgb_class_wghts,\n",
        "#                           param_grid=param_grid,\n",
        "#                           cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "#                           n_jobs=-1,\n",
        "#                           scoring='f1',\n",
        "#                           verbose=2).fit(X_train, y_train_array)\n",
        "\n",
        "# # Create a DataFrame for plotting\n",
        "# weigh_data = pd.DataFrame({'score': gridsearch.cv_results_['mean_test_score'], 'weight': (1 - weights)})\n",
        "\n",
        "# # Create an interactive line plot\n",
        "# fig = px.line(weigh_data, x='weight', y='score',\n",
        "#               labels={'weight': 'Weight for class 1', 'score': 'F1 score'},\n",
        "#               title='Scoring for different class weights',\n",
        "#               width=800, height=500)\n",
        "\n",
        "# # Display the interactive plot\n",
        "# fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK-i66pzcuSA"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_child_weight': [1, 3, 5, 10],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3, 0.4]\n",
        "}\n",
        "\n",
        "# XGBoost model creation\n",
        "xgb_class_wghts = XGBClassifier(n_jobs=-1, scale_pos_weight=0.22 / 0.78, random_state=123)\n",
        "\n",
        "best_params = GridSearch(xgb_class_wghts, param_grid, 3, X_train, y_train_array)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Create XGBoost model with the best hyperparameters and class weights\n",
        "xgb_class_wghts = XGBClassifier(\n",
        "    max_depth=best_params['max_depth'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    min_child_weight=best_params['min_child_weight'],\n",
        "    subsample=best_params['subsample'],\n",
        "    colsample_bytree=best_params['colsample_bytree'],\n",
        "    gamma=best_params['gamma'],\n",
        "    scale_pos_weight=0.22 / 0.78,  # Set scale_pos_weight based on the class weights\n",
        "    n_jobs=-1\n",
        "    )\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_class_wghts.fit(X_train, y_train_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTU_xK6dccnm"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_train_array.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGHXJV3qccnm"
      },
      "outputs": [],
      "source": [
        "type(y_train_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgipbXKoccnm"
      },
      "outputs": [],
      "source": [
        "y_train_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-56B1a-8eZa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFmIg6yVnE2N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE  # Import SMOTE if not already imported\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Model 1: Original RandomForestClassifier\n",
        "y_pred_org = rfc_org.predict(X_test)\n",
        "\n",
        "# Model 2: RandomForestClassifier with SMOTE\n",
        "y_pred_smote = rfc_SMOTE.predict(X_test)\n",
        "\n",
        "# Model 3: RandomForestClassifier with class weights\n",
        "y_pred_class_wghts = rfc_class_wghts.predict(X_test)\n",
        "\n",
        "# Model 4: XgBoost Classifier with class weights\n",
        "y_pred_xgb_org = xgb_org.predict(X_test)\n",
        "\n",
        "# Model 5: RandomForestClassifier with class weights\n",
        "y_pred_xgb_SMOTE = xgb_SMOTE.predict(X_test)\n",
        "\n",
        "# Model 6: XgBoost Classifier with class weights\n",
        "y_pred_xgb_class_wghts = xgb_class_wghts.predict(X_test)\n",
        "\n",
        "# Compare metrics for each model\n",
        "models = ['rfc_org', 'rfc_SMOTE', 'rfc_class_wghts', 'xgb_org', 'xgb_SMOTE', 'xgb_class_wghts']\n",
        "\n",
        "\n",
        "for model_name, y_pred in zip(models, [y_pred_org, y_pred_smote, y_pred_class_wghts, y_pred_xgb_org, y_pred_xgb_SMOTE, y_pred_xgb_class_wghts]):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{model_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "    # Additional metrics\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nClassification Report for {model_name}:\\n{classification_rep}\")\n",
        "    print(f\"Confusion Matrix for {model_name}:\\n{confusion_mat}\")\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiQ0_M0g1Euc"
      },
      "outputs": [],
      "source": [
        "###################### Model Performances on Train Data ######################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Model predictions on train data\n",
        "y_pred_org_train = rfc_org.predict(X_train)\n",
        "y_pred_smote_train = rfc_SMOTE.predict(X_train)\n",
        "y_pred_class_wghts_train = rfc_class_wghts.predict(X_train)\n",
        "y_pred_xgb_org_train = xgb_org.predict(X_train)\n",
        "y_pred_xgb_SMOTE_train = xgb_SMOTE.predict(X_train)\n",
        "y_pred_xgb_class_wghts_train = xgb_class_wghts.predict(X_train)\n",
        "\n",
        "# Compare metrics for each model on train data\n",
        "models = ['rfc_org', 'rfc_SMOTE', 'rfc_class_wghts', 'xgb_org', 'xgb_SMOTE', 'xgb_class_wghts']\n",
        "\n",
        "# Initialize empty lists to store metrics\n",
        "accuracy_list_train = []\n",
        "precision_list_0_train, precision_list_1_train = [], []\n",
        "recall_list_0_train, recall_list_1_train = [], []\n",
        "f1_list_0_train, f1_list_1_train = [], []\n",
        "\n",
        "for model_name, y_pred_train in zip(models, [y_pred_org_train, y_pred_smote_train, y_pred_class_wghts_train, y_pred_xgb_org_train, y_pred_xgb_SMOTE_train, y_pred_xgb_class_wghts_train]):\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "    precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_pred_train, average=None)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_list_train.append(accuracy_train)\n",
        "    precision_list_0_train.append(precision_train[0])\n",
        "    precision_list_1_train.append(precision_train[1])\n",
        "    recall_list_0_train.append(recall_train[0])\n",
        "    recall_list_1_train.append(recall_train[1])\n",
        "    f1_list_0_train.append(f1_train[0])\n",
        "    f1_list_1_train.append(f1_train[1])\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"{model_name} Metrics on Train Data:\")\n",
        "    print(f\"Accuracy: {accuracy_train:.4f}\")\n",
        "    print(f\"Precision (Class 0): {precision_train[0]:.4f}\")\n",
        "    print(f\"Precision (Class 1): {precision_train[1]:.4f}\")\n",
        "    print(f\"Recall (Class 0): {recall_train[0]:.4f}\")\n",
        "    print(f\"Recall (Class 1): {recall_train[1]:.4f}\")\n",
        "    print(f\"F1-score (Class 0): {f1_train[0]:.4f}\")\n",
        "    print(f\"F1-score (Class 1): {f1_train[1]:.4f}\")\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "metrics_list = [accuracy_list_train, precision_list_0_train, precision_list_1_train, recall_list_0_train, recall_list_1_train, f1_list_0_train, f1_list_1_train]\n",
        "metric_labels = ['Accuracy', 'Precision (Class 0)', 'Precision (Class 1)', 'Recall (Class 0)', 'Recall (Class 1)', 'F1-score (Class 0)', 'F1-score (Class 1)']\n",
        "\n",
        "# Plot each metric on separate subplots\n",
        "for i, (metric_values, metric_label) in enumerate(zip(metrics_list, metric_labels)):\n",
        "    row, col = divmod(i, 4)\n",
        "    axes[row, col].bar(models, metric_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "    axes[row, col].set_ylabel(metric_label, fontsize=12)  # Adjust font size here\n",
        "    axes[row, col].set_title(f'{metric_label} Comparison', fontsize=14)  # Adjust font size here\n",
        "    axes[row, col].tick_params(axis='x', labelrotation=45)  # Rotate x-axis labels for better readability\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS_CoZFF1uCC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5PvmbqU1uTJ"
      },
      "source": [
        "Let's analyze the results for the loan approval classification task based on the provided metrics for each model:\n",
        "\n",
        "1. **rfc_org (Random Forest Classifier without any modifications):**\n",
        "   - **Accuracy:** 74.07%\n",
        "   - **Precision (Class 0):** 88.54%\n",
        "   - **Precision (Class 1):** 73.85%\n",
        "   - **Recall (Class 0):** 4.90%\n",
        "   - **Recall (Class 1):** 99.76%\n",
        "   - **F1-score (Class 0):** 9.29%\n",
        "   - **F1-score (Class 1):** 84.88%\n",
        "\n",
        "2. **rfc_SMOTE (Random Forest Classifier with SMOTE):**\n",
        "   - **Accuracy:** 69.10%\n",
        "   - **Precision (Class 0):** 41.81%\n",
        "   - **Precision (Class 1):** 77.40%\n",
        "   - **Recall (Class 0):** 36.00%\n",
        "   - **Recall (Class 1):** 81.39%\n",
        "   - **F1-score (Class 0):** 38.69%\n",
        "   - **F1-score (Class 1):** 79.34%\n",
        "\n",
        "3. **rfc_class_wghts (Random Forest Classifier with class weights):**\n",
        "   - **Accuracy:** 74.70%\n",
        "   - **Precision (Class 0):** 64.23%\n",
        "   - **Precision (Class 1):** 75.40%\n",
        "   - **Recall (Class 0):** 14.88%\n",
        "   - **Recall (Class 1):** 96.92%\n",
        "   - **F1-score (Class 0):** 24.16%\n",
        "   - **F1-score (Class 1):** 84.82%\n",
        "\n",
        "**Analysis:**\n",
        "- **Accuracy:** rfc_class_wghts has the highest accuracy (74.70%), followed by rfc_org (74.07%), and then rfc_SMOTE (69.10%).\n",
        "- **Precision:** rfc_org has the highest precision for both classes. However, rfc_class_wghts has a reasonable precision while considering both classes.\n",
        "- **Recall:** rfc_org has very low recall for Class 0, while rfc_class_wghts has improved recall for both classes. rfc_SMOTE has a balanced recall for both classes but is lower compared to rfc_class_wghts.\n",
        "- **F1-score:** rfc_org has the highest F1-score for Class 1, while rfc_class_wghts has a better balance between precision and recall for both classes.\n",
        "\n",
        "**Conclusion:**\n",
        "- Overall, considering the trade-off between precision, recall, and F1-score, the model **rfc_class_wghts** seems to be performing better on the training data for the loan approval classification task. It has a good balance between precision and recall for both classes, resulting in a higher F1-score. The choice of the best model also depends on the specific goals and requirements of your application. If balancing precision and recall is crucial, rfc_class_wghts might be a better choice despite having a lower accuracy compared to rfc_org."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r9zgk9Uucbl"
      },
      "outputs": [],
      "source": [
        "###################### Model Performances on Test Data ######################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Model predictions\n",
        "y_pred_org = rfc_org.predict(X_test)\n",
        "y_pred_smote = rfc_SMOTE.predict(X_test)\n",
        "y_pred_class_wghts = rfc_class_wghts.predict(X_test)\n",
        "y_pred_xgb_org = xgb_org.predict(X_test)\n",
        "y_pred_xgb_SMOTE = xgb_SMOTE.predict(X_test)\n",
        "y_pred_xgb_class_wghts = xgb_class_wghts.predict(X_test)\n",
        "\n",
        "# Compare metrics for each model\n",
        "models = ['rfc_org', 'rfc_SMOTE', 'rfc_class_wghts', 'xgb_org', 'xgb_SMOTE', 'xgb_class_wghts']\n",
        "\n",
        "# Initialize empty lists to store metrics\n",
        "accuracy_list = []\n",
        "precision_list_0, precision_list_1 = [], []\n",
        "recall_list_0, recall_list_1 = [], []\n",
        "f1_list_0, f1_list_1 = [], []\n",
        "\n",
        "for model_name, y_pred in zip(models, [y_pred_org, y_pred_smote, y_pred_class_wghts, y_pred_xgb_org, y_pred_xgb_SMOTE, y_pred_xgb_class_wghts]):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list_0.append(precision[0])\n",
        "    precision_list_1.append(precision[1])\n",
        "    recall_list_0.append(recall[0])\n",
        "    recall_list_1.append(recall[1])\n",
        "    f1_list_0.append(f1[0])\n",
        "    f1_list_1.append(f1[1])\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"=======Model Performance on test data=======\")\n",
        "    print(f\"{model_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision (Class 0): {precision[0]:.4f}\")\n",
        "    print(f\"Precision (Class 1): {precision[1]:.4f}\")\n",
        "    print(f\"Recall (Class 0): {recall[0]:.4f}\")\n",
        "    print(f\"Recall (Class 1): {recall[1]:.4f}\")\n",
        "    print(f\"F1-score (Class 0): {f1[0]:.4f}\")\n",
        "    print(f\"F1-score (Class 1): {f1[1]:.4f}\")\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "metrics_list = [accuracy_list, precision_list_0, precision_list_1, recall_list_0, recall_list_1, f1_list_0, f1_list_1]\n",
        "metric_labels = ['Accuracy', 'Precision (Class 0)', 'Precision (Class 1)', 'Recall (Class 0)', 'Recall (Class 1)', 'F1-score (Class 0)', 'F1-score (Class 1)']\n",
        "\n",
        "# Plot each metric on separate subplots\n",
        "for i, (metric_values, metric_label) in enumerate(zip(metrics_list, metric_labels)):\n",
        "    row, col = divmod(i, 4)\n",
        "    axes[row, col].bar(models, metric_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "    axes[row, col].set_ylabel(metric_label, fontsize=12)  # Adjust font size here\n",
        "    axes[row, col].set_title(f'{metric_label} Comparison', fontsize=14)  # Adjust font size here\n",
        "    axes[row, col].tick_params(axis='x', labelrotation=45)  # Rotate x-axis labels for better readability\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uWDCykNyiDJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ru7_INMzKEa"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXPpMgPs1_XK"
      },
      "source": [
        "Let's analyze the results for the loan approval classification task based on the provided metrics for each model:\n",
        "\n",
        "1. **rfc_org (Random Forest Classifier without any modifications):**\n",
        "   - **Accuracy:** 74.23%\n",
        "   - **Precision (Class 0):** 92.50%\n",
        "   - **Precision (Class 1):** 73.94%\n",
        "   - **Recall (Class 0):** 5.22%\n",
        "   - **Recall (Class 1):** 99.84%\n",
        "   - **F1-score (Class 0):** 9.88%\n",
        "   - **F1-score (Class 1):** 84.96%\n",
        "\n",
        "2. **rfc_SMOTE (Random Forest Classifier with SMOTE):**\n",
        "   - **Accuracy:** 66.59%\n",
        "   - **Precision (Class 0):** 37.31%\n",
        "   - **Precision (Class 1):** 76.34%\n",
        "   - **Recall (Class 0):** 34.41%\n",
        "   - **Recall (Class 1):** 78.53%\n",
        "   - **F1-score (Class 0):** 35.80%\n",
        "   - **F1-score (Class 1):** 77.42%\n",
        "\n",
        "3. **rfc_class_wghts (Random Forest Classifier with class weights):**\n",
        "   - **Accuracy:** 74.26%\n",
        "   - **Precision (Class 0):** 61.44%\n",
        "   - **Precision (Class 1):** 75.06%\n",
        "   - **Recall (Class 0):** 13.26%\n",
        "   - **Recall (Class 1):** 96.91%\n",
        "   - **F1-score (Class 0):** 21.81%\n",
        "   - **F1-score (Class 1):** 84.60%\n",
        "\n",
        "**Analysis:**\n",
        "- **Accuracy:** rfc_class_wghts has a slightly higher accuracy (74.26%), followed by rfc_org (74.23%), and then rfc_SMOTE (66.59%).\n",
        "- **Precision:** rfc_org has the highest precision for both classes. rfc_class_wghts has a reasonable precision while considering both classes.\n",
        "- **Recall:** rfc_org has very low recall for Class 0, while rfc_class_wghts has improved recall for both classes. rfc_SMOTE has a balanced recall for both classes but is lower compared to rfc_class_wghts.\n",
        "- **F1-score:** rfc_org has the highest F1-score for Class 1, while rfc_class_wghts has a better balance between precision and recall for both classes.\n",
        "\n",
        "**Conclusion:**\n",
        "- Similar to the previous analysis, considering the trade-off between precision, recall, and F1-score, the model **rfc_class_wghts** seems to be performing better on the loan approval classification task. It has a good balance between precision and recall for both classes, resulting in a higher F1-score. The choice of the best model also depends on the specific goals and requirements of your application. If balancing precision and recall is crucial, rfc_class_wghts might be a better choice despite having a slightly lower accuracy compared to rfc_org."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAvZefMf1_1L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPC8mE-qzLq9"
      },
      "source": [
        "Let's analyze the performance chart for your Loan Approval classifier. The chart compares four different models: `RFC_org`, `RFC_SMOTE`, `RFC_class_weights`, and `XGB_class_weights`. Here are the insights:\n",
        "\n",
        "1. **Accuracy**:\n",
        "   - Among the models, `XGB_class_weights` has the highest accuracy, followed by `RFC_SMOTE`.\n",
        "   - Accuracy measures overall correctness of predictions.\n",
        "\n",
        "2. **Precision (Class 0)**:\n",
        "   - `XGB_class_weights` excels in precision for classifying loan denials (class 0).\n",
        "   - Precision indicates how well the model predicts true negatives (loan denials).\n",
        "\n",
        "3. **Precision (Class 1)**:\n",
        "   - Once again, `XGB_class_weights` performs best in precision for classifying loan approvals (class 1).\n",
        "   - Precision reflects the model's ability to predict true positives (loan approvals).\n",
        "\n",
        "4. **Recall (Class 0)**:\n",
        "   - `RFC_org` achieves the highest recall rate for classifying loan denials.\n",
        "   - Recall captures the proportion of actual loan denials correctly predicted by the model.\n",
        "\n",
        "5. **Recall (Class 1)**:\n",
        "   - `XGB_class_weights` outperforms other models in recall rate for classifying loan approvals.\n",
        "   - Recall represents the proportion of actual loan approvals correctly predicted.\n",
        "\n",
        "6. **F1-score (Class 0)**:\n",
        "   - `XGB_class_weights` has the best F1-score for classifying loan denials.\n",
        "   - F1-score balances precision and recall for class 0 predictions.\n",
        "\n",
        "7. **F1-score (Class 1)**:\n",
        "   - Once again, `XGB_class_weights` leads in F1-score, indicating balanced performance for classifying loan approvals.\n",
        "\n",
        "**Final Verdict**:\n",
        "Based on the overall performance across all metrics, especially with higher accuracy and balanced F1-scores for both classes, I recommend using the `XGB_class_weights` model for your Loan Approval task. It seems to strike a good balance between precision and recall for both loan approvals and denials. 🚀📊"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Plot AUC Curve on Train Data ##########################\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Models and labels\n",
        "models = ['rfc_org', 'rfc_SMOTE', 'rfc_class_wghts', 'xgb_org', 'xgb_SMOTE', 'xgb_class_wghts']\n",
        "labels = ['Random Forest (Original)', 'Random Forest (SMOTE)', 'Random Forest (Class Weights)', 'XGBoost Org','XGBoost SMOTE','XGBoost (Class Weights)']\n",
        "\n",
        "# Plotting for all models\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle(\"ROC Curve for all Models on Train Data\")\n",
        "\n",
        "# Iterate through models\n",
        "for model, label, ax in zip(models, labels, axs.flatten()):\n",
        "    y_pred_proba = model.predict_proba(X_train)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_train.values, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    ax.plot(fpr, tpr, ls=\"--\", label=f\"{label} AUC = {roc_auc:.2f}\")\n",
        "\n",
        "    # Plot no skill line\n",
        "    ax.plot([0, 1], [0, 1], c=\"r\", label=\"No Skill AUC = 0.5\")\n",
        "\n",
        "    # Set plot labels and legend\n",
        "    ax.legend(loc=\"upper left\", fontsize='x-small')\n",
        "    ax.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3qU4Zedtq1y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Plot AUC Curve on Test Data ##########################\n",
        "# Models and labels\n",
        "models = ['rfc_org', 'rfc_SMOTE', 'rfc_class_wghts', 'xgb_org', 'xgb_SMOTE', 'xgb_class_wghts']\n",
        "labels = ['Random Forest (Original)', 'Random Forest (SMOTE)', 'Random Forest (Class Weights)', 'XGBoost Org','XGBoost SMOTE','XGBoost (Class Weights)']\n",
        "\n",
        "# Plotting for all models\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle(\"ROC Curve for all Models on Test Data\")\n",
        "\n",
        "# Iterate through models\n",
        "for model, label, ax in zip(models, labels, axs.flatten()):\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test.values, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    ax.plot(fpr, tpr, ls=\"--\", label=f\"{label} AUC = {roc_auc:.2f}\")\n",
        "\n",
        "    # Plot no skill line\n",
        "    ax.plot([0, 1], [0, 1], c=\"r\", label=\"No Skill AUC = 0.5\")\n",
        "\n",
        "    # Set plot labels and legend\n",
        "    ax.legend(loc=\"upper left\", fontsize='x-small')\n",
        "    ax.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eTb7xhONr4Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5QJoFnFccnp"
      },
      "source": [
        "The term \"No Skill\" in the context of ROC-AUC refers to a classifier that makes random predictions or predictions that do not take the class distribution into account. It's essentially a baseline for comparison. A model that has no discriminatory power and predicts classes randomly would have an AUC of 0.5. In other words, an AUC of 0.5 indicates that the model performs no better than random chance.\n",
        "\n",
        "Here's a brief interpretation of AUC values:\n",
        "\n",
        "- AUC = 0.5: No discriminatory power (equivalent to random guessing).\n",
        "- 0.5 < AUC < 1: Better than random chance; the higher the AUC, the better the model discriminates between positive and negative samples.\n",
        "- AUC = 1: Perfect discriminatory power; the model perfectly separates positive and negative samples.\n",
        "\n",
        "Therefore, when interpreting the ROC-AUC values for your models:\n",
        "\n",
        "- AUC around 0.5 suggests poor performance or a model that doesn't provide much information beyond random chance.\n",
        "- AUC significantly above 0.5 indicates better-than-random performance.\n",
        "- AUC close to 1 suggests strong discriminatory power.\n",
        "\n",
        "In your ROC curve plot, the line for \"No Skill\" is a straight line from the origin to (1, 1) with an AUC of 0.5, providing a baseline for comparison with the models' performance curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNOfviMGccnq"
      },
      "outputs": [],
      "source": [
        "###################################### Precision VS Recall plot for all Models on Train Data ######################################\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import pandas as pd\n",
        "\n",
        "# Models and labels\n",
        "models = ['rfc_org', 'rfc_SMOTE', 'rfc_class_wghts', 'xgb_org', 'xgb_SMOTE', 'xgb_class_wghts']\n",
        "labels = ['Random Forest (Original)', 'Random Forest (SMOTE)', 'Random Forest (Class Weights)', 'XGBoost Org','XGBoost SMOTE','XGBoost (Class Weights)']\n",
        "\n",
        "# Plotting for all models\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle(\"Precision VS Recall plot for all Models on Train Data\")\n",
        "\n",
        "# Iterate through models\n",
        "for model, label, ax in zip(models, labels, axs.flatten()):\n",
        "    y_pred_prob = model.predict_proba(X_train)[:, 1]\n",
        "    precision, recall, threshold = precision_recall_curve(y_train.values, y_pred_prob)\n",
        "    prt_df = pd.DataFrame({\n",
        "        \"threshold\": threshold,\n",
        "        \"recall\": recall[1:],\n",
        "        \"precision\": precision[1:]\n",
        "    })\n",
        "    prt_melted = pd.melt(prt_df, id_vars=[\"threshold\"], value_vars=[\"recall\", \"precision\"])\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    sns.lineplot(x=\"threshold\", y=\"value\", hue=\"variable\", data=prt_melted, ax=ax)\n",
        "    ax.set_title(label)  # Add title for each subplot\n",
        "\n",
        "# Overall plot settings\n",
        "for ax in axs.flat:\n",
        "    ax.set(xlabel='Threshold', ylabel='Score')\n",
        "    ax.legend(title='Metrics', loc='upper right', fontsize='x-small')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_label,metric_values,accuracy_list_train"
      ],
      "metadata": {
        "id": "41O_8AUxxG_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ############################## Model Performance on Train Data with custom threshold value ##############################\n",
        "# from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# # Custom thresholds for each model\n",
        "# thresholds = {'rfc_org': 0.7, 'rfc_SMOTE': 0.5, 'rfc_class_wghts': 0.42, 'xgb_class_wghts': 0.38}\n",
        "\n",
        "# # Initialize empty lists to store metrics\n",
        "# accuracy_list_train = []\n",
        "# precision_list_0_train, precision_list_1_train = [], []\n",
        "# recall_list_0_train, recall_list_1_train = [], []\n",
        "# f1_list_0_train, f1_list_1_train = [], []\n",
        "\n",
        "# # Initialize labels using thresholds.keys()\n",
        "# labels = list(thresholds.keys())\n",
        "\n",
        "# for model, label in zip(models, labels):\n",
        "#     threshold = thresholds[label]\n",
        "\n",
        "#     # Predict train data\n",
        "#     y_pred_train = model.predict(X_train)\n",
        "\n",
        "#     # Apply custom threshold\n",
        "#     y_pred_train_custom = (y_pred_train > threshold).astype(int)\n",
        "#     print(\"y_train.shape, y_pred_train_custom.shape:\", y_train.shape, y_pred_train_custom.shape)\n",
        "\n",
        "#     accuracy_train = accuracy_score(y_train, y_pred_train_custom)\n",
        "#     precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_pred_train_custom, average=None)\n",
        "\n",
        "#     # Append metrics to lists\n",
        "#     accuracy_list_train.append(accuracy_train)\n",
        "#     precision_list_0_train.append(precision_train[0])\n",
        "#     precision_list_1_train.append(precision_train[1])\n",
        "#     recall_list_0_train.append(recall_train[0])\n",
        "#     recall_list_1_train.append(recall_train[1])\n",
        "#     f1_list_0_train.append(f1_train[0])\n",
        "#     f1_list_1_train.append(f1_train[1])\n",
        "\n",
        "#     # Print metrics\n",
        "#     print(f\"{label} Metrics on Train Data with Custom Threshold {threshold}:\")\n",
        "#     print(f\"Accuracy: {accuracy_train:.4f}\")\n",
        "#     print(f\"Precision (Class 0): {precision_train[0]:.4f}\")\n",
        "#     print(f\"Precision (Class 1): {precision_train[1]:.4f}\")\n",
        "#     print(f\"Recall (Class 0): {recall_train[0]:.4f}\")\n",
        "#     print(f\"Recall (Class 1): {recall_train[1]:.4f}\")\n",
        "#     print(f\"F1-score (Class 0): {f1_train[0]:.4f}\")\n",
        "#     print(f\"F1-score (Class 1): {f1_train[1]:.4f}\")\n",
        "#     print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# # Plotting\n",
        "# fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "# metrics_list = [accuracy_list_train, precision_list_0_train, precision_list_1_train, recall_list_0_train, recall_list_1_train, f1_list_0_train, f1_list_1_train]\n",
        "# metric_labels = ['Accuracy', 'Precision (Class 0)', 'Precision (Class 1)', 'Recall (Class 0)', 'Recall (Class 1)', 'F1-score (Class 0)', 'F1-score (Class 1)']\n",
        "\n",
        "# # Plot each metric on separate subplots\n",
        "# for i, (metric_values, metric_label) in enumerate(zip(metrics_list, metric_labels)):\n",
        "#     row, col = divmod(i, 4)\n",
        "#     axes[row, col].bar(labels, metric_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "#     axes[row, col].set_ylabel(metric_label, fontsize=12)  # Adjust font size here\n",
        "#     axes[row, col].set_title(f'{metric_label} Comparison', fontsize=14)  # Adjust font size here\n",
        "#     axes[row, col].tick_params(axis='x', labelrotation=45)  # Rotate x-axis labels for better readability\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "CL_eWACgkZBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ############################## Model Performance on Test Data with custom threshold value ##############################\n",
        "\n",
        "# from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# # Custom thresholds for each model\n",
        "# thresholds = {'rfc_org': 0.7, 'rfc_SMOTE': 0.5, 'rfc_class_wghts': 0.42, 'xgb_class_wghts': 0.38}\n",
        "\n",
        "# # Initialize empty lists to store metrics\n",
        "# accuracy_list_test = []\n",
        "# precision_list_0_test, precision_list_1_test = [], []\n",
        "# recall_list_0_test, recall_list_1_test = [], []\n",
        "# f1_list_0_test, f1_list_1_test = [], []\n",
        "\n",
        "# # Initialize labels using thresholds.keys()\n",
        "# labels = list(thresholds.keys())\n",
        "\n",
        "# for model, label in zip(models, labels):\n",
        "#     threshold = thresholds[label]\n",
        "\n",
        "#     # Predict train data\n",
        "#     y_pred_test = model.predict(X_test)\n",
        "\n",
        "#     # Apply custom threshold\n",
        "#     y_pred_test_custom = (y_pred_test > threshold).astype(int)\n",
        "#     print(\"y_test.shape, y_pred_test_custom.shape:\", y_test.shape, y_pred_test_custom.shape)\n",
        "\n",
        "#     accuracy_test = accuracy_score(y_test, y_pred_test_custom)\n",
        "#     precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_pred_test_custom, average=None)\n",
        "\n",
        "#     # Append metrics to lists\n",
        "#     accuracy_list_test.append(accuracy_test)\n",
        "#     precision_list_0_test.append(precision_test[0])\n",
        "#     precision_list_1_test.append(precision_test[1])\n",
        "#     recall_list_0_test.append(recall_test[0])\n",
        "#     recall_list_1_test.append(recall_test[1])\n",
        "#     f1_list_0_test.append(f1_test[0])\n",
        "#     f1_list_1_test.append(f1_test[1])\n",
        "\n",
        "#     # Print metrics\n",
        "#     print(f\"{label} Metrics on Train Data with Custom Threshold {threshold}:\")\n",
        "#     print(f\"Accuracy: {accuracy_test:.4f}\")\n",
        "#     print(f\"Precision (Class 0): {precision_test[0]:.4f}\")\n",
        "#     print(f\"Precision (Class 1): {precision_test[1]:.4f}\")\n",
        "#     print(f\"Recall (Class 0): {recall_test[0]:.4f}\")\n",
        "#     print(f\"Recall (Class 1): {recall_test[1]:.4f}\")\n",
        "#     print(f\"F1-score (Class 0): {f1_test[0]:.4f}\")\n",
        "#     print(f\"F1-score (Class 1): {f1_test[1]:.4f}\")\n",
        "#     print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# # Plotting\n",
        "# fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "# metrics_list = [accuracy_list_test, precision_list_0_test, precision_list_1_test, recall_list_0_test, recall_list_1_test, f1_list_0_test, f1_list_1_test]\n",
        "# metric_labels = ['Accuracy', 'Precision (Class 0)', 'Precision (Class 1)', 'Recall (Class 0)', 'Recall (Class 1)', 'F1-score (Class 0)', 'F1-score (Class 1)']\n",
        "\n",
        "# # Plot each metric on separate subplots\n",
        "# for i, (metric_values, metric_label) in enumerate(zip(metrics_list, metric_labels)):\n",
        "#     row, col = divmod(i, 4)\n",
        "#     axes[row, col].bar(labels, metric_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "#     axes[row, col].set_ylabel(metric_label, fontsize=12)  # Adjust font size here\n",
        "#     axes[row, col].set_title(f'{metric_label} Comparison', fontsize=14)  # Adjust font size here\n",
        "#     axes[row, col].tick_params(axis='x', labelrotation=45)  # Rotate x-axis labels for better readability\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "Us4m9ryop2yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# Stop code execution\n",
        "sys.exit()\n"
      ],
      "metadata": {
        "id": "WuR8RQqxdJ7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYuTRJrhzPD9"
      },
      "outputs": [],
      "source": [
        "########################### SHAP for model explanation ################\n",
        "#Initiate SHAP explainer and compute SHAP Values\n",
        "import shap\n",
        "shap.initjs()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeHHQ5j14FGI"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import display, HTML\n",
        "# display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js'></script>\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQLIdAubz4Qf"
      },
      "outputs": [],
      "source": [
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(rfc_class_wghts)\n",
        "shap_values = explainer.shap_values(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePLWmV8zccnq"
      },
      "outputs": [],
      "source": [
        "len(shap_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NYFbnJnccnq"
      },
      "outputs": [],
      "source": [
        "##############Summary Plot################\n",
        "shap.summary_plot(shap_values, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92SLnycSccnr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnypvMklccnr"
      },
      "source": [
        "Let's analyze the above chart:\n",
        "\n",
        "1. **Feature Importance Analysis**:\n",
        "   - The chart represents the **mean SHAP (SHapley Additive exPlanations)** values for various features in a **Loan Approval Classifier**.\n",
        "   - Each feature's impact on the model's output (loan approval or denial) is visualized.\n",
        "   - Features are listed vertically along the y-axis, and their importance is represented by the length of the bars.\n",
        "   - The x-axis represents the **mean(|SHAP value|)**, indicating the average impact on the model's output magnitude.\n",
        "\n",
        "2. **Specific Insights**:\n",
        "   - Let's focus on the following features:\n",
        "     - **LoanPurpose_Refinance**:\n",
        "       - This feature has the most significant positive impact on **Class 1** (loan approval).\n",
        "       - Borrowers seeking loans for refinancing purposes are more likely to be approved.\n",
        "     - **BorrowerOwnRent_Rent**:\n",
        "       - Also positively influences **Class 1** but to a lesser extent than LoanPurpose_Refinance.\n",
        "       - Borrowers who rent (rather than own) their residence have a favorable impact on loan approval.\n",
        "     - **CLTV (Collateral Loan-to-Value Ratio)**:\n",
        "       - Has mixed impacts on both classes.\n",
        "       - High CLTV may increase risk (negative impact on Class 1).\n",
        "     - **BorrowerAge**:\n",
        "       - Mixed impact, affecting both classes.\n",
        "       - Younger or older borrowers may have varying effects.\n",
        "     - **DTI (Debt-to-Income Ratio)**:\n",
        "       - Also has mixed impacts.\n",
        "       - High DTI may negatively influence loan approval.\n",
        "     - **TotalIncome**:\n",
        "       - Mostly impacts **Class 0** (loan denial) negatively.\n",
        "       - Higher total income improves chances of approval.\n",
        "     - **ZipCode_77**, **ZipCode_78**, and **ZipCode_76**:\n",
        "       - Different zip codes have varying effects on both classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FjEqjjdccnr"
      },
      "outputs": [],
      "source": [
        "# Display the summary_plot of the label “0”.\n",
        "shap.summary_plot(shap_values[0], X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDpTO4Ipi_EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlRO7ykUccnr"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values[1], X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y552LK7Lccnr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlAe81Rkccnr"
      },
      "source": [
        "1. **Positive Impact Features (Blue)**:\n",
        "   - **LoanPurpose_Refinance**: Borrowers seeking loans for refinancing purposes have a strong positive impact on loan approval (class 1).\n",
        "   - **CreditScore**: Higher credit scores also positively influence loan approval, although there is some variability.\n",
        "\n",
        "2. **Negative Impact Features (Pink/Red)**:\n",
        "   - **BorrowerOwnRent_Rent and CLTV**: These features strongly contribute to loan denial (class 0).\n",
        "   - **DTI (Debt-to-Income Ratio)**: High DTI negatively affects loan approval.\n",
        "   - **TotalIncome**: Lower total income is associated with loan denial.\n",
        "   - **LeadSourceGroup_Repeat Client**: Repeat clients have a negative impact on loan approval.\n",
        "   - **ZipCode_77**, **ZipCode_78**, and **ZipCode_76**: Different zip codes influence loan approval differently.\n",
        "   - **TotalLoanAmount**, **LoanType_FHA/VA**, and **Education_UnderGrad/PostGrad/PHD**: These features also contribute negatively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm2TbrPlccns"
      },
      "outputs": [],
      "source": [
        "# Visualize the `dependence_plot` between the feature “Subscription Length” and “Age.”\n",
        "\n",
        "\n",
        "shap.dependence_plot(\"CreditScore\", shap_values[1], X_train_bkp,interaction_index=\"BorrowerAge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iLIb2Ryccns"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCkfKKNQccns"
      },
      "source": [
        "The **SHAP (SHapley Additive exPlanations)** dependence plot you provided illustrates the relationship between the **Credit Score** and the **SHAP value for Credit Score**, with color coding based on **Borrower Age**. Let's break down the insights:\n",
        "\n",
        "1. **X-Axis (CreditScore)**:\n",
        "   - Represents the credit score of borrowers.\n",
        "   - Ranges from approximately 550 to 800.\n",
        "\n",
        "2. **Y-Axis (SHAP value for CreditScore)**:\n",
        "   - Represents the impact of credit score on the model's prediction.\n",
        "   - Ranges from about -0.14 to 0.\n",
        "   - Positive values indicate a positive effect on the model's output, while negative values indicate a negative effect.\n",
        "\n",
        "3. **Color Coding (BorrowerAge)**:\n",
        "   - The color bar on the right side of the plot represents borrower age.\n",
        "   - Blue corresponds to younger borrowers (around age 35), and red corresponds to older borrowers (around age 70).\n",
        "\n",
        "4. **Observations**:\n",
        "   - For higher credit scores (around 700 and above), the SHAP values are mostly clustered around 0. This suggests that credit scores in this range have little impact on the model's output.\n",
        "   - Within the credit score range of 550 to 650, there is a noticeable negative correlation with SHAP values. As credit scores increase, the SHAP value becomes less negative. This implies that increasing credit scores within this range positively affect the model’s prediction.\n",
        "   - Borrowers aged around 35-45 tend to have lower SHAP values at lower credit scores.\n",
        "\n",
        "5. **Final Verdict**:\n",
        "   - Increasing credit scores within the range of 550 to 650 can lead to improved model predictions.\n",
        "   - However, for higher credit scores, the impact on the model's output is minimal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y35W3Hcnccnt"
      },
      "outputs": [],
      "source": [
        "# Force Plot\n",
        "# We will examine the first sample in the testing set to determine which features contributed to the \"0\" result.\n",
        "# To do this, we will utilize a force plot and provide the expected value, SHAP value, and testing sample\n",
        "shap.plots.force(explainer.expected_value[0], shap_values[0][0,:], X_train_bkp.iloc[0, :], matplotlib = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CztoktIDccnt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KUiyIttccnt"
      },
      "source": [
        "The **SHAP (SHapley Additive exPlanations)** force plot you provided visualizes the contribution of each feature to a specific prediction. Let's delve into the insights:\n",
        "\n",
        "1. **Base Value**:\n",
        "   - The base value (shown as **0.56**) represents the starting point before considering individual feature impacts.\n",
        "\n",
        "2. **Feature Contributions**:\n",
        "   - Features pushing the prediction **higher** (to the right) are highlighted in **red**.\n",
        "   - Features pushing the prediction **lower** (to the left) are highlighted in **blue**.\n",
        "\n",
        "3. **Feature Insights**:\n",
        "   - **Positive Contributors** (in red):\n",
        "       - **TotalIncome**: A higher total income of **$4490.89** contributes positively.\n",
        "       - **CLTV (Combined Loan-to-Value ratio)**: A CLTV of **69** also pushes the score higher.\n",
        "       - **LoanPurpose_Refinance**: If the loan purpose is refinance, it positively impacts the prediction.\n",
        "   - **Negative Contributors** (in blue):\n",
        "       - **BorrowerAge**: A borrower's age of **47** negatively impacts the prediction.\n",
        "       - **DTI (Debt-to-Income ratio)**: A DTI of **41** also pulls the score down.\n",
        "       - **ZipCode_77**: The presence of this zip code feature has a negative effect.\n",
        "\n",
        "4. **Final Verdict**:\n",
        "   - After considering both positive and negative contributions, the **predicted score** for this observation is approximately **0.58**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWuQbBKq0UC0"
      },
      "outputs": [],
      "source": [
        "# Let’s look at customer churn samples with label “1”.\n",
        "\n",
        "shap.plots.force(explainer.expected_value[1], shap_values[1][6, :], X_train_bkp.iloc[6, :],matplotlib = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OcQsnkLNiJn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **SHAP (SHapley Additive exPlanations)** force plot you provided visualizes the contribution of each feature to a specific prediction. Let's delve into the insights:\n",
        "\n",
        "1. **Base Value**:\n",
        "   - The base value (shown as **0.44**) represents the starting point before considering individual feature impacts.\n",
        "\n",
        "2. **Feature Contributions**:\n",
        "   - Features pushing the prediction **higher** (to the right) are highlighted in **red**.\n",
        "   - Features pushing the prediction **lower** (to the left) are highlighted in **blue**.\n",
        "\n",
        "3. **Feature Insights**:\n",
        "   - **Positive Contributors** (in red):\n",
        "       - **LoanPurpose_Refinance**: If the loan purpose is refinance, it positively impacts the prediction.\n",
        "       - **LTV (Loan-to-Value ratio)**: A higher LTV (approximately 68.33) contributes positively.\n",
        "       - **BorrowerOwnRent_Rent**: Borrowers who rent (rather than own) positively influence the prediction.\n",
        "   - **Negative Contributors** (in blue):\n",
        "       - **ZipCode_78** and **ZipCode_77**: These zip codes have a negative effect on the prediction.\n",
        "       - **TotalLoanAmount**: A higher total loan amount (approximately $410,000) pulls the score down.\n",
        "       - **TotalIncome**: Borrowers with a total income of $11,250 negatively impact the prediction.\n",
        "       - **BorrowerAge**: A borrower's age of 52 also lowers the prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "4iNU0oitiKRB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uANXnUMEccnt"
      },
      "outputs": [],
      "source": [
        "# Select the first record from X_test and its corresponding SHAP values\n",
        "first_record_X = X_train_bkp.iloc[[0], :]\n",
        "first_record_shap_values = shap_values[1][0, :]\n",
        "\n",
        "# Decision Plot for the first record\n",
        "shap.decision_plot(\n",
        "    explainer.expected_value[1],\n",
        "    first_record_shap_values.reshape(1, -1),  # Reshape to a matrix with one row\n",
        "    feature_names = list(first_record_X.columns)\n",
        ",  # Use feature names from the DataFrame\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATHShzg6fyFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **SHAP (SHapley Additive exPlanations)** decision plot you provided visualizes the contribution of each feature to a specific prediction for the first record. Let's break down the insights:\n",
        "\n",
        "1. **Base Value**:\n",
        "   - The base value (shown as **0.44**) represents the starting point before considering individual feature impacts.\n",
        "\n",
        "2. **Feature Contributions**:\n",
        "   - Features pushing the prediction **higher** (to the right) are highlighted in **red**.\n",
        "   - Features pushing the prediction **lower** (to the left) are highlighted in **blue**.\n",
        "\n",
        "3. **Feature Insights**:\n",
        "   - **Positive Contributors** (in red):\n",
        "       - **LoanPurpose_Refinance**: If the loan purpose is refinance, it positively impacts the prediction.\n",
        "       - **LTV (Loan-to-Value ratio)**: A higher LTV (approximately 68.33) contributes positively.\n",
        "       - **BorrowerOwnRent_Rent**: Borrowers who rent (rather than own) positively influence the prediction.\n",
        "   - **Negative Contributors** (in blue):\n",
        "       - **ZipCode_78** and **ZipCode_77**: These zip codes have a negative effect on the prediction.\n",
        "       - **TotalLoanAmount**: A higher total loan amount (approximately $410,000) pulls the score down.\n",
        "       - **TotalIncome**: Borrowers with a total income of $11,250 negatively impact the prediction.\n",
        "       - **BorrowerAge**: A borrower's age of 52 also lowers the prediction.\n",
        "\n",
        "4. **Final Verdict**:\n",
        "   - After considering both positive and negative contributions, the **predicted score** for this observation is approximately **0.42**.\n"
      ],
      "metadata": {
        "id": "gVMdVC0Kfy3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0PHt9a2ccnv"
      },
      "outputs": [],
      "source": [
        "# Get indexes of class 1 in y_test\n",
        "class_1_indexes = y_train[y_train[\"Approved\"] == 0].index\n",
        "\n",
        "# Subsample X_test based on class 1 indexes\n",
        "first_100_records_X = X_train_bkp.reset_index()[0:100]\n",
        "first_100_records_shap_values = shap_values[0][0:100]\n",
        "\n",
        "# Decision Plot for the first 100 records\n",
        "shap.decision_plot(\n",
        "    explainer.expected_value[1],\n",
        "    first_100_records_shap_values,  # No need to reshape for multiple records\n",
        "    feature_names=list(X_test_bkp.columns),  # Use feature names from the DataFrame\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0DwIHdhhn_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **SHAP (SHapley Additive exPlanations)** decision plot you provided visualizes the contribution of each feature to the model's output for multiple records. Let's break down the insights:\n",
        "\n",
        "1. **Base Value**:\n",
        "   - The base value represents the starting point before considering individual feature impacts.\n",
        "\n",
        "2. **Feature Contributions**:\n",
        "   - Features pushing the prediction **higher** (to the right) are highlighted in **red**.\n",
        "   - Features pushing the prediction **lower** (to the left) are highlighted in **blue**.\n",
        "\n",
        "3. **Feature Insights**:\n",
        "   - **Positive Contributors** (in red):\n",
        "       - **LoanPurpose_Refinance**: If the loan purpose is refinance, it positively impacts the prediction.\n",
        "       - **CLTV (Combined Loan-to-Value ratio)**: A higher CLTV contributes positively.\n",
        "       - **BorrowerOwnRent_Rent**: Borrowers who rent (rather than own) positively influence the prediction.\n",
        "   - **Negative Contributors** (in blue):\n",
        "       - **DTI (Debt-to-Income ratio)**: A higher DTI pulls the score down.\n",
        "       - **TotalIncome**: Borrowers with higher total income negatively impact the prediction.\n",
        "       - **BorrowerAge**: A borrower's age also lowers the prediction.\n",
        "\n",
        "4. **Final Verdict**:\n",
        "   - After considering both positive and negative contributions, the **predicted score** for these observations tends to vary based on the specific feature values.\n",
        "\n"
      ],
      "metadata": {
        "id": "zgo8o9NJhode"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvvKrET4ccnv"
      },
      "outputs": [],
      "source": [
        "# Get indexes of class 1 in y_test\n",
        "class_1_indexes = y_train[y_train[\"Approved\"] == 1].index\n",
        "\n",
        "# Subsample X_test based on class 1 indexes\n",
        "first_100_records_X = X_train_bkp.reset_index()[0:100]\n",
        "first_100_records_shap_values = shap_values[1][0:100]\n",
        "\n",
        "# Decision Plot for the first 100 records\n",
        "shap.decision_plot(\n",
        "    explainer.expected_value[1],\n",
        "    first_100_records_shap_values,  # No need to reshape for multiple records\n",
        "    feature_names=list(X_test_bkp.columns),  # Use feature names from the DataFrame\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoclkI8kccnv"
      },
      "outputs": [],
      "source": [
        "# shap.force_plot(explainer.expected_value, shap_values.values, X_test, feature_names = catVars+numVars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJHMb1JFccnw"
      },
      "outputs": [],
      "source": [
        "# shap.plots.heatmap(shap_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4kxOurwccnw"
      },
      "outputs": [],
      "source": [
        "# shap.plots.beeswarm(shap_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7THMSkaVccnw"
      },
      "outputs": [],
      "source": [
        "# shap.plots.bar(shap_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQjaqw1Eccnw"
      },
      "outputs": [],
      "source": [
        "# SHAP values for all predictions\n",
        "# shap.force_plot(explainer.expected_value[1], shap_values[1], X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiDvm3yUccnw"
      },
      "outputs": [],
      "source": [
        "# shap.summary_plot(shap_values, X, plot_type=\"bar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fTYVCvzHh-ks"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}